Thinking1	参数共享指的是什么？
Answer1    卷积核中的参数在不同的图像位置上使用的参数都是一样的

Thinking2	为什么会用到batch normalization ?
Answer2    在深度学习中，学习的层数会很深，为了更快加速收敛，需要把输入值的分布强行拉回到均值为0方差为1的标准正态分布，是后续的学习效率提高加快训练速度，避免梯度消失的问题

Thinking3	使用dropout可以解决什么问题？
Answer3    dropout是卷积核参数在不同位置权重共享，可以关闭掉一些神经元，在下一次的输出结果中不进行任何操作，可以防止过拟合，让模型更加泛化

