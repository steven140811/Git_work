{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ResNet&DenseNet_Cifar10_EarlyStopping.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "632984ba8f2248b0a6974d42ff160c67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_eb6149495ed544518e66df3fff6bab4f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_ccee170be3d042c896bcccf711475590",
              "IPY_MODEL_9c0640833b22475e9fc03d2f809d9121"
            ]
          }
        },
        "eb6149495ed544518e66df3fff6bab4f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ccee170be3d042c896bcccf711475590": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_1333b0435ed048baa2f76bc9187cfb61",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 102502400,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 102502400,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f2557af796ca4a79bd91c31611f5efd5"
          }
        },
        "9c0640833b22475e9fc03d2f809d9121": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_71695543edbc402cb3eba52c1a74c8f8",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 97.8M/97.8M [3:27:26&lt;00:00, 8.24kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_77641d817cb84dd5b333a5a32910f6cf"
          }
        },
        "1333b0435ed048baa2f76bc9187cfb61": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f2557af796ca4a79bd91c31611f5efd5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "71695543edbc402cb3eba52c1a74c8f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "77641d817cb84dd5b333a5a32910f6cf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cb60ba58354046aba656fab1f27bcd6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_94eed83e949540699894a8874bffc5fd",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_6674dea4a09f431b98113d7b8823603a",
              "IPY_MODEL_b83ab6f6ed364eb7b2188c84afbcaec0"
            ]
          }
        },
        "94eed83e949540699894a8874bffc5fd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6674dea4a09f431b98113d7b8823603a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_027cfc5f0a7b432fbf4cf26aeeb49cf4",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 32342954,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 32342954,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6baaae45f9b044aeb0290e550568507c"
          }
        },
        "b83ab6f6ed364eb7b2188c84afbcaec0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_402c6e3907e5477d87e3abcae262fdc7",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 30.8M/30.8M [00:00&lt;00:00, 112MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_252cc046295047d3babb14ba8125dc36"
          }
        },
        "027cfc5f0a7b432fbf4cf26aeeb49cf4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6baaae45f9b044aeb0290e550568507c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "402c6e3907e5477d87e3abcae262fdc7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "252cc046295047d3babb14ba8125dc36": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4895e98e91364499901a5bcb0c0b13a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_9b49249894cd45c0a85cc1438c8e14e8",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_62c150fda84e422bbeb4af1eb47d5569",
              "IPY_MODEL_a624cd6f577c4d4fb3e635fd14b9dee4"
            ]
          }
        },
        "9b49249894cd45c0a85cc1438c8e14e8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "62c150fda84e422bbeb4af1eb47d5569": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_7ca4e13fb13342a8b83f7e67b08eabbe",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 81131730,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 81131730,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c376c99546a94f79a76c2bc74275a05a"
          }
        },
        "a624cd6f577c4d4fb3e635fd14b9dee4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d42e84542463476fb229e9534adf8d15",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 77.4M/77.4M [00:01&lt;00:00, 69.3MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a9c5809c0b844e8397e66bf77911fc87"
          }
        },
        "7ca4e13fb13342a8b83f7e67b08eabbe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c376c99546a94f79a76c2bc74275a05a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d42e84542463476fb229e9534adf8d15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a9c5809c0b844e8397e66bf77911fc87": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0rJ8newcvQo",
        "outputId": "05baf124-6525-48ca-ad98-24bca9567177",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import os\n",
        "os.chdir(\"/content/drive/My Drive/Colab Notebooks/\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mt4-Oy31hnnF"
      },
      "source": [
        "# RESNET50的方式对CIFAR10进行分类预测"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbBM70QedRUE",
        "outputId": "7ab25d43-309a-46f1-eccf-16269f5de580",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "632984ba8f2248b0a6974d42ff160c67",
            "eb6149495ed544518e66df3fff6bab4f",
            "ccee170be3d042c896bcccf711475590",
            "9c0640833b22475e9fc03d2f809d9121",
            "1333b0435ed048baa2f76bc9187cfb61",
            "f2557af796ca4a79bd91c31611f5efd5",
            "71695543edbc402cb3eba52c1a74c8f8",
            "77641d817cb84dd5b333a5a32910f6cf"
          ]
        }
      },
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.transforms import ToPILImage\n",
        "%matplotlib inline\n",
        "import time\n",
        "show = ToPILImage() #可以把Tensor转换成Image,方便可视化\n",
        "transform = transforms.Compose([  #transforms.Compose就是将对图像处理的方法集中起来\n",
        "    transforms.RandomHorizontalFlip(),#水平翻转\n",
        "    transforms.RandomCrop((32, 32), padding=4),#对图片进行随机裁剪，裁剪的大小是32*32的，填充是4\n",
        "#     参数：size- (sequence or int)，若为sequence,则为(h,w)，若为int，则(size,size)\n",
        "#     padding-(sequence or int, optional)，此参数是设置填充多少个pixel。\n",
        "#     当为int时，图像上下左右均填充int个，例如padding=4，则上下左右均填充4个pixel，若为32x32，则会变成40x40。\n",
        "#     当为sequence时，若有2个数，则第一个数表示左右扩充多少，第二个数表示上下的。当有4个数时，则为左，上，右，下。\n",
        "#     fill- (int or tuple) 填充的值是什么（仅当填充模式为constant时有用）。int时，各通道均填充该值，当长度为3的tuple时，表示RGB通道需要填充的值。\n",
        "    transforms.ToTensor(),#转为Tensor\n",
        "    #在做数据归一化之前必须要把PIL Image转成Tensor，而其他resize或crop操作则不需要。\n",
        "     transforms.Normalize((0.5, 0.5 ,0.5), (0.5, 0.5, 0.5)),#归一化\n",
        "#     归一化操作，这里有两个参数一个是均值，一个是方差，由于是RGB型的所以一个参有三个值，这里面的参数的大小是可调的，调节的公式是：\n",
        "#     class torchvision.transforms.Normalize(mean, std)，\n",
        "#     Normalized_image=(image-mean)/std\n",
        "#     channel=（channel-mean）/std(因为transforms.ToTensor()已经把数据处理成[0,1],那么(x-0.5)/0.5\n",
        "#     就是[-1.0, 1.0])这样一来，我们的数据中的每个值就变成了[-1,1]的数了。\n",
        "    ])\n",
        "\n",
        "# 超参数定义\n",
        "EPOCH = 300               # 训练epoch次数\n",
        "BATCH_SIZE = 200         # 批训练的数量\n",
        "LR = 0.0001              # 学习率\n",
        "DOWNLOAD_MNIST = False  # 设置True 可以自动下载数据\n",
        "\n",
        "# CIFAR-10数据集下载\n",
        "train_data = datasets.CIFAR10(root='/content/drive/My Drive/Colab Notebooks/datas/datasets_data',\n",
        "                         train=True,                         # 训练集\n",
        "                         # 数据变换(0, 255) -> (0, 1)\n",
        "                         transform=transform   \n",
        "                           )\n",
        "\n",
        "test_data = datasets.CIFAR10(root='/content/drive/My Drive/Colab Notebooks/datas/datasets_data',\n",
        "                        train=False,                         # 测试集\n",
        "                        transform=transform\n",
        "                        #download=True\n",
        "                          )\n",
        "\n",
        "# 使用DataLoader进行分批\n",
        "train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_data, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "#数据集10个类的定义\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "# resnet50 Model\n",
        "model = torchvision.models.resnet50(pretrained=True)\n",
        "#损失函数:这里用交叉熵\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "#优化器 Adam\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "#device : GPU or CPU\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "print('Start Training')\n",
        "# 训练\n",
        "\n",
        "for epoch in range(EPOCH):\n",
        "    running_loss = 0.0\n",
        "    start = time.time()\n",
        "    for i, data in enumerate(train_loader):#enumerate枚举数据并从下标0开始\n",
        "        # 读取数据的数据内容和标签\n",
        "        #print('读取数据的数据内容和标签')\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        # 前向传播\n",
        "        #print('前向传播')\n",
        "        outputs = model(inputs)\n",
        "        # 计算损失函数\n",
        "        #print('计算损失函数')\n",
        "        loss = criterion(outputs, labels)\n",
        "        #梯度清零，也就是把loss关于weight的导数变成0.\n",
        "        optimizer.zero_grad()\n",
        "        # 反向传播\n",
        "        loss.backward()\n",
        "        # 参数更新\n",
        "        optimizer.step()\n",
        "        # #打印log信息\n",
        "        running_loss += loss.item()# #用于从tensor中获取python数字\n",
        "        if i % 50 == 49:#每50个batch打印一次训练状态\n",
        "            print('[%d, %5d] loss: %.3f' \\\n",
        "                 % (epoch+1, i+1, running_loss / 2000))\n",
        "\n",
        "            running_loss = 0.0\n",
        "#print('Finished Training')\n",
        "    end = time.time()\n",
        "    print('epoch{} loss:{:.4f} using time is {:.2f}s'.format(epoch+1, loss.item(), (end-start)))\n",
        "    if loss.item() < 0.001:\n",
        "      print('Training Stop')\n",
        "      break\n",
        "print(\"Finished Traning\")\n",
        "\n",
        "\n",
        "#保存训练模型\n",
        "torch.save(model, '/content/drive/My Drive/Colab Notebooks/datas/datasets_data/cifar_10_resnet.pt')\n",
        "model = torch.load('/content/drive/My Drive/Colab Notebooks/datas/datasets_data/cifar_10_resnet.pt')\n",
        "\n",
        "# 测试\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "for data in test_loader:\n",
        "    images, labels = data\n",
        "    images, labels = images.to(device), labels.to(device)\n",
        "    # 前向传播\n",
        "    out = model(images)\n",
        "    _, predicted = torch.max(out.data, 1)\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "\n",
        "#输出识别准确率\n",
        "print('10000测试图像 准确率:{:.4f}%'.format(100 * correct / total)) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-19c8e357.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "632984ba8f2248b0a6974d42ff160c67",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=102502400.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Start Training\n",
            "[1,    50] loss: 0.121\n",
            "[1,   100] loss: 0.041\n",
            "[1,   150] loss: 0.031\n",
            "[1,   200] loss: 0.027\n",
            "[1,   250] loss: 0.024\n",
            "epoch1 loss:1.0223 using time is 55.84s\n",
            "[2,    50] loss: 0.022\n",
            "[2,   100] loss: 0.021\n",
            "[2,   150] loss: 0.020\n",
            "[2,   200] loss: 0.020\n",
            "[2,   250] loss: 0.019\n",
            "epoch2 loss:0.5170 using time is 57.72s\n",
            "[3,    50] loss: 0.017\n",
            "[3,   100] loss: 0.016\n",
            "[3,   150] loss: 0.016\n",
            "[3,   200] loss: 0.015\n",
            "[3,   250] loss: 0.015\n",
            "epoch3 loss:0.4664 using time is 57.13s\n",
            "[4,    50] loss: 0.014\n",
            "[4,   100] loss: 0.014\n",
            "[4,   150] loss: 0.013\n",
            "[4,   200] loss: 0.013\n",
            "[4,   250] loss: 0.013\n",
            "epoch4 loss:0.4888 using time is 56.69s\n",
            "[5,    50] loss: 0.012\n",
            "[5,   100] loss: 0.012\n",
            "[5,   150] loss: 0.012\n",
            "[5,   200] loss: 0.011\n",
            "[5,   250] loss: 0.012\n",
            "epoch5 loss:0.4119 using time is 56.90s\n",
            "[6,    50] loss: 0.010\n",
            "[6,   100] loss: 0.010\n",
            "[6,   150] loss: 0.010\n",
            "[6,   200] loss: 0.011\n",
            "[6,   250] loss: 0.011\n",
            "epoch6 loss:0.4968 using time is 56.68s\n",
            "[7,    50] loss: 0.009\n",
            "[7,   100] loss: 0.009\n",
            "[7,   150] loss: 0.009\n",
            "[7,   200] loss: 0.009\n",
            "[7,   250] loss: 0.009\n",
            "epoch7 loss:0.3121 using time is 56.67s\n",
            "[8,    50] loss: 0.008\n",
            "[8,   100] loss: 0.009\n",
            "[8,   150] loss: 0.008\n",
            "[8,   200] loss: 0.008\n",
            "[8,   250] loss: 0.008\n",
            "epoch8 loss:0.3293 using time is 56.37s\n",
            "[9,    50] loss: 0.007\n",
            "[9,   100] loss: 0.007\n",
            "[9,   150] loss: 0.008\n",
            "[9,   200] loss: 0.007\n",
            "[9,   250] loss: 0.008\n",
            "epoch9 loss:0.2232 using time is 56.64s\n",
            "[10,    50] loss: 0.007\n",
            "[10,   100] loss: 0.007\n",
            "[10,   150] loss: 0.007\n",
            "[10,   200] loss: 0.007\n",
            "[10,   250] loss: 0.007\n",
            "epoch10 loss:0.3794 using time is 56.50s\n",
            "[11,    50] loss: 0.006\n",
            "[11,   100] loss: 0.006\n",
            "[11,   150] loss: 0.006\n",
            "[11,   200] loss: 0.006\n",
            "[11,   250] loss: 0.007\n",
            "epoch11 loss:0.3667 using time is 56.30s\n",
            "[12,    50] loss: 0.005\n",
            "[12,   100] loss: 0.006\n",
            "[12,   150] loss: 0.006\n",
            "[12,   200] loss: 0.006\n",
            "[12,   250] loss: 0.006\n",
            "epoch12 loss:0.2188 using time is 56.23s\n",
            "[13,    50] loss: 0.005\n",
            "[13,   100] loss: 0.005\n",
            "[13,   150] loss: 0.006\n",
            "[13,   200] loss: 0.005\n",
            "[13,   250] loss: 0.006\n",
            "epoch13 loss:0.2995 using time is 56.37s\n",
            "[14,    50] loss: 0.004\n",
            "[14,   100] loss: 0.005\n",
            "[14,   150] loss: 0.005\n",
            "[14,   200] loss: 0.005\n",
            "[14,   250] loss: 0.005\n",
            "epoch14 loss:0.1802 using time is 56.36s\n",
            "[15,    50] loss: 0.004\n",
            "[15,   100] loss: 0.004\n",
            "[15,   150] loss: 0.004\n",
            "[15,   200] loss: 0.005\n",
            "[15,   250] loss: 0.005\n",
            "epoch15 loss:0.1332 using time is 56.25s\n",
            "[16,    50] loss: 0.004\n",
            "[16,   100] loss: 0.004\n",
            "[16,   150] loss: 0.005\n",
            "[16,   200] loss: 0.004\n",
            "[16,   250] loss: 0.005\n",
            "epoch16 loss:0.2125 using time is 56.35s\n",
            "[17,    50] loss: 0.004\n",
            "[17,   100] loss: 0.004\n",
            "[17,   150] loss: 0.004\n",
            "[17,   200] loss: 0.004\n",
            "[17,   250] loss: 0.004\n",
            "epoch17 loss:0.1913 using time is 56.22s\n",
            "[18,    50] loss: 0.003\n",
            "[18,   100] loss: 0.004\n",
            "[18,   150] loss: 0.004\n",
            "[18,   200] loss: 0.004\n",
            "[18,   250] loss: 0.004\n",
            "epoch18 loss:0.1746 using time is 56.31s\n",
            "[19,    50] loss: 0.003\n",
            "[19,   100] loss: 0.003\n",
            "[19,   150] loss: 0.004\n",
            "[19,   200] loss: 0.004\n",
            "[19,   250] loss: 0.004\n",
            "epoch19 loss:0.1471 using time is 56.46s\n",
            "[20,    50] loss: 0.003\n",
            "[20,   100] loss: 0.003\n",
            "[20,   150] loss: 0.003\n",
            "[20,   200] loss: 0.003\n",
            "[20,   250] loss: 0.004\n",
            "epoch20 loss:0.1582 using time is 56.24s\n",
            "[21,    50] loss: 0.003\n",
            "[21,   100] loss: 0.003\n",
            "[21,   150] loss: 0.003\n",
            "[21,   200] loss: 0.003\n",
            "[21,   250] loss: 0.003\n",
            "epoch21 loss:0.1409 using time is 56.38s\n",
            "[22,    50] loss: 0.003\n",
            "[22,   100] loss: 0.003\n",
            "[22,   150] loss: 0.003\n",
            "[22,   200] loss: 0.003\n",
            "[22,   250] loss: 0.003\n",
            "epoch22 loss:0.1003 using time is 56.24s\n",
            "[23,    50] loss: 0.002\n",
            "[23,   100] loss: 0.003\n",
            "[23,   150] loss: 0.003\n",
            "[23,   200] loss: 0.003\n",
            "[23,   250] loss: 0.003\n",
            "epoch23 loss:0.0891 using time is 56.37s\n",
            "[24,    50] loss: 0.002\n",
            "[24,   100] loss: 0.003\n",
            "[24,   150] loss: 0.003\n",
            "[24,   200] loss: 0.003\n",
            "[24,   250] loss: 0.003\n",
            "epoch24 loss:0.0862 using time is 56.10s\n",
            "[25,    50] loss: 0.002\n",
            "[25,   100] loss: 0.002\n",
            "[25,   150] loss: 0.002\n",
            "[25,   200] loss: 0.003\n",
            "[25,   250] loss: 0.003\n",
            "epoch25 loss:0.0780 using time is 56.38s\n",
            "[26,    50] loss: 0.002\n",
            "[26,   100] loss: 0.002\n",
            "[26,   150] loss: 0.002\n",
            "[26,   200] loss: 0.003\n",
            "[26,   250] loss: 0.003\n",
            "epoch26 loss:0.1081 using time is 56.14s\n",
            "[27,    50] loss: 0.002\n",
            "[27,   100] loss: 0.002\n",
            "[27,   150] loss: 0.002\n",
            "[27,   200] loss: 0.002\n",
            "[27,   250] loss: 0.003\n",
            "epoch27 loss:0.0775 using time is 56.22s\n",
            "[28,    50] loss: 0.002\n",
            "[28,   100] loss: 0.002\n",
            "[28,   150] loss: 0.002\n",
            "[28,   200] loss: 0.002\n",
            "[28,   250] loss: 0.003\n",
            "epoch28 loss:0.0494 using time is 56.28s\n",
            "[29,    50] loss: 0.002\n",
            "[29,   100] loss: 0.002\n",
            "[29,   150] loss: 0.002\n",
            "[29,   200] loss: 0.002\n",
            "[29,   250] loss: 0.002\n",
            "epoch29 loss:0.0576 using time is 56.23s\n",
            "[30,    50] loss: 0.002\n",
            "[30,   100] loss: 0.002\n",
            "[30,   150] loss: 0.002\n",
            "[30,   200] loss: 0.002\n",
            "[30,   250] loss: 0.002\n",
            "epoch30 loss:0.0973 using time is 55.97s\n",
            "[31,    50] loss: 0.002\n",
            "[31,   100] loss: 0.002\n",
            "[31,   150] loss: 0.002\n",
            "[31,   200] loss: 0.002\n",
            "[31,   250] loss: 0.002\n",
            "epoch31 loss:0.0690 using time is 55.98s\n",
            "[32,    50] loss: 0.002\n",
            "[32,   100] loss: 0.002\n",
            "[32,   150] loss: 0.002\n",
            "[32,   200] loss: 0.002\n",
            "[32,   250] loss: 0.002\n",
            "epoch32 loss:0.0727 using time is 56.46s\n",
            "[33,    50] loss: 0.002\n",
            "[33,   100] loss: 0.002\n",
            "[33,   150] loss: 0.002\n",
            "[33,   200] loss: 0.002\n",
            "[33,   250] loss: 0.002\n",
            "epoch33 loss:0.1237 using time is 56.57s\n",
            "[34,    50] loss: 0.002\n",
            "[34,   100] loss: 0.002\n",
            "[34,   150] loss: 0.002\n",
            "[34,   200] loss: 0.002\n",
            "[34,   250] loss: 0.002\n",
            "epoch34 loss:0.1187 using time is 56.57s\n",
            "[35,    50] loss: 0.001\n",
            "[35,   100] loss: 0.002\n",
            "[35,   150] loss: 0.002\n",
            "[35,   200] loss: 0.002\n",
            "[35,   250] loss: 0.002\n",
            "epoch35 loss:0.0994 using time is 56.58s\n",
            "[36,    50] loss: 0.002\n",
            "[36,   100] loss: 0.002\n",
            "[36,   150] loss: 0.002\n",
            "[36,   200] loss: 0.002\n",
            "[36,   250] loss: 0.002\n",
            "epoch36 loss:0.1034 using time is 56.17s\n",
            "[37,    50] loss: 0.001\n",
            "[37,   100] loss: 0.001\n",
            "[37,   150] loss: 0.002\n",
            "[37,   200] loss: 0.002\n",
            "[37,   250] loss: 0.002\n",
            "epoch37 loss:0.1521 using time is 56.09s\n",
            "[38,    50] loss: 0.001\n",
            "[38,   100] loss: 0.002\n",
            "[38,   150] loss: 0.001\n",
            "[38,   200] loss: 0.002\n",
            "[38,   250] loss: 0.002\n",
            "epoch38 loss:0.0487 using time is 56.21s\n",
            "[39,    50] loss: 0.001\n",
            "[39,   100] loss: 0.002\n",
            "[39,   150] loss: 0.002\n",
            "[39,   200] loss: 0.002\n",
            "[39,   250] loss: 0.002\n",
            "epoch39 loss:0.0749 using time is 56.23s\n",
            "[40,    50] loss: 0.001\n",
            "[40,   100] loss: 0.001\n",
            "[40,   150] loss: 0.001\n",
            "[40,   200] loss: 0.002\n",
            "[40,   250] loss: 0.001\n",
            "epoch40 loss:0.0364 using time is 56.21s\n",
            "[41,    50] loss: 0.001\n",
            "[41,   100] loss: 0.001\n",
            "[41,   150] loss: 0.001\n",
            "[41,   200] loss: 0.001\n",
            "[41,   250] loss: 0.002\n",
            "epoch41 loss:0.0243 using time is 56.13s\n",
            "[42,    50] loss: 0.002\n",
            "[42,   100] loss: 0.001\n",
            "[42,   150] loss: 0.001\n",
            "[42,   200] loss: 0.002\n",
            "[42,   250] loss: 0.002\n",
            "epoch42 loss:0.1060 using time is 56.07s\n",
            "[43,    50] loss: 0.001\n",
            "[43,   100] loss: 0.001\n",
            "[43,   150] loss: 0.002\n",
            "[43,   200] loss: 0.001\n",
            "[43,   250] loss: 0.001\n",
            "epoch43 loss:0.0570 using time is 56.04s\n",
            "[44,    50] loss: 0.001\n",
            "[44,   100] loss: 0.001\n",
            "[44,   150] loss: 0.001\n",
            "[44,   200] loss: 0.001\n",
            "[44,   250] loss: 0.002\n",
            "epoch44 loss:0.0648 using time is 56.17s\n",
            "[45,    50] loss: 0.001\n",
            "[45,   100] loss: 0.001\n",
            "[45,   150] loss: 0.002\n",
            "[45,   200] loss: 0.002\n",
            "[45,   250] loss: 0.001\n",
            "epoch45 loss:0.0680 using time is 56.28s\n",
            "[46,    50] loss: 0.001\n",
            "[46,   100] loss: 0.001\n",
            "[46,   150] loss: 0.001\n",
            "[46,   200] loss: 0.001\n",
            "[46,   250] loss: 0.001\n",
            "epoch46 loss:0.0611 using time is 56.21s\n",
            "[47,    50] loss: 0.001\n",
            "[47,   100] loss: 0.001\n",
            "[47,   150] loss: 0.001\n",
            "[47,   200] loss: 0.001\n",
            "[47,   250] loss: 0.001\n",
            "epoch47 loss:0.0658 using time is 56.21s\n",
            "[48,    50] loss: 0.001\n",
            "[48,   100] loss: 0.001\n",
            "[48,   150] loss: 0.001\n",
            "[48,   200] loss: 0.001\n",
            "[48,   250] loss: 0.001\n",
            "epoch48 loss:0.0424 using time is 56.03s\n",
            "[49,    50] loss: 0.001\n",
            "[49,   100] loss: 0.001\n",
            "[49,   150] loss: 0.001\n",
            "[49,   200] loss: 0.001\n",
            "[49,   250] loss: 0.001\n",
            "epoch49 loss:0.1158 using time is 56.12s\n",
            "[50,    50] loss: 0.001\n",
            "[50,   100] loss: 0.001\n",
            "[50,   150] loss: 0.001\n",
            "[50,   200] loss: 0.001\n",
            "[50,   250] loss: 0.001\n",
            "epoch50 loss:0.0625 using time is 56.39s\n",
            "[51,    50] loss: 0.001\n",
            "[51,   100] loss: 0.001\n",
            "[51,   150] loss: 0.001\n",
            "[51,   200] loss: 0.001\n",
            "[51,   250] loss: 0.001\n",
            "epoch51 loss:0.0633 using time is 56.34s\n",
            "[52,    50] loss: 0.001\n",
            "[52,   100] loss: 0.001\n",
            "[52,   150] loss: 0.001\n",
            "[52,   200] loss: 0.001\n",
            "[52,   250] loss: 0.001\n",
            "epoch52 loss:0.0273 using time is 56.12s\n",
            "[53,    50] loss: 0.001\n",
            "[53,   100] loss: 0.001\n",
            "[53,   150] loss: 0.001\n",
            "[53,   200] loss: 0.001\n",
            "[53,   250] loss: 0.001\n",
            "epoch53 loss:0.0678 using time is 56.18s\n",
            "[54,    50] loss: 0.001\n",
            "[54,   100] loss: 0.001\n",
            "[54,   150] loss: 0.001\n",
            "[54,   200] loss: 0.001\n",
            "[54,   250] loss: 0.001\n",
            "epoch54 loss:0.0469 using time is 56.00s\n",
            "[55,    50] loss: 0.001\n",
            "[55,   100] loss: 0.001\n",
            "[55,   150] loss: 0.001\n",
            "[55,   200] loss: 0.001\n",
            "[55,   250] loss: 0.001\n",
            "epoch55 loss:0.0228 using time is 56.21s\n",
            "[56,    50] loss: 0.001\n",
            "[56,   100] loss: 0.001\n",
            "[56,   150] loss: 0.001\n",
            "[56,   200] loss: 0.001\n",
            "[56,   250] loss: 0.001\n",
            "epoch56 loss:0.0566 using time is 56.49s\n",
            "[57,    50] loss: 0.001\n",
            "[57,   100] loss: 0.001\n",
            "[57,   150] loss: 0.001\n",
            "[57,   200] loss: 0.001\n",
            "[57,   250] loss: 0.001\n",
            "epoch57 loss:0.0894 using time is 56.13s\n",
            "[58,    50] loss: 0.001\n",
            "[58,   100] loss: 0.001\n",
            "[58,   150] loss: 0.001\n",
            "[58,   200] loss: 0.001\n",
            "[58,   250] loss: 0.001\n",
            "epoch58 loss:0.0497 using time is 56.28s\n",
            "[59,    50] loss: 0.001\n",
            "[59,   100] loss: 0.001\n",
            "[59,   150] loss: 0.001\n",
            "[59,   200] loss: 0.001\n",
            "[59,   250] loss: 0.001\n",
            "epoch59 loss:0.0956 using time is 56.21s\n",
            "[60,    50] loss: 0.001\n",
            "[60,   100] loss: 0.001\n",
            "[60,   150] loss: 0.001\n",
            "[60,   200] loss: 0.001\n",
            "[60,   250] loss: 0.001\n",
            "epoch60 loss:0.0492 using time is 56.16s\n",
            "[61,    50] loss: 0.001\n",
            "[61,   100] loss: 0.001\n",
            "[61,   150] loss: 0.001\n",
            "[61,   200] loss: 0.001\n",
            "[61,   250] loss: 0.001\n",
            "epoch61 loss:0.0369 using time is 56.03s\n",
            "[62,    50] loss: 0.001\n",
            "[62,   100] loss: 0.001\n",
            "[62,   150] loss: 0.001\n",
            "[62,   200] loss: 0.001\n",
            "[62,   250] loss: 0.001\n",
            "epoch62 loss:0.0862 using time is 56.18s\n",
            "[63,    50] loss: 0.001\n",
            "[63,   100] loss: 0.001\n",
            "[63,   150] loss: 0.001\n",
            "[63,   200] loss: 0.001\n",
            "[63,   250] loss: 0.001\n",
            "epoch63 loss:0.0275 using time is 56.31s\n",
            "[64,    50] loss: 0.001\n",
            "[64,   100] loss: 0.001\n",
            "[64,   150] loss: 0.001\n",
            "[64,   200] loss: 0.001\n",
            "[64,   250] loss: 0.001\n",
            "epoch64 loss:0.0552 using time is 56.16s\n",
            "[65,    50] loss: 0.001\n",
            "[65,   100] loss: 0.001\n",
            "[65,   150] loss: 0.001\n",
            "[65,   200] loss: 0.001\n",
            "[65,   250] loss: 0.001\n",
            "epoch65 loss:0.0578 using time is 56.03s\n",
            "[66,    50] loss: 0.001\n",
            "[66,   100] loss: 0.001\n",
            "[66,   150] loss: 0.001\n",
            "[66,   200] loss: 0.001\n",
            "[66,   250] loss: 0.001\n",
            "epoch66 loss:0.0258 using time is 56.06s\n",
            "[67,    50] loss: 0.001\n",
            "[67,   100] loss: 0.001\n",
            "[67,   150] loss: 0.001\n",
            "[67,   200] loss: 0.001\n",
            "[67,   250] loss: 0.001\n",
            "epoch67 loss:0.0169 using time is 56.50s\n",
            "[68,    50] loss: 0.001\n",
            "[68,   100] loss: 0.001\n",
            "[68,   150] loss: 0.001\n",
            "[68,   200] loss: 0.001\n",
            "[68,   250] loss: 0.001\n",
            "epoch68 loss:0.0779 using time is 56.41s\n",
            "[69,    50] loss: 0.001\n",
            "[69,   100] loss: 0.001\n",
            "[69,   150] loss: 0.001\n",
            "[69,   200] loss: 0.001\n",
            "[69,   250] loss: 0.001\n",
            "epoch69 loss:0.0252 using time is 56.23s\n",
            "[70,    50] loss: 0.001\n",
            "[70,   100] loss: 0.001\n",
            "[70,   150] loss: 0.001\n",
            "[70,   200] loss: 0.001\n",
            "[70,   250] loss: 0.001\n",
            "epoch70 loss:0.0219 using time is 56.31s\n",
            "[71,    50] loss: 0.001\n",
            "[71,   100] loss: 0.001\n",
            "[71,   150] loss: 0.001\n",
            "[71,   200] loss: 0.001\n",
            "[71,   250] loss: 0.001\n",
            "epoch71 loss:0.0393 using time is 56.32s\n",
            "[72,    50] loss: 0.001\n",
            "[72,   100] loss: 0.001\n",
            "[72,   150] loss: 0.001\n",
            "[72,   200] loss: 0.001\n",
            "[72,   250] loss: 0.001\n",
            "epoch72 loss:0.0423 using time is 56.37s\n",
            "[73,    50] loss: 0.001\n",
            "[73,   100] loss: 0.001\n",
            "[73,   150] loss: 0.001\n",
            "[73,   200] loss: 0.001\n",
            "[73,   250] loss: 0.001\n",
            "epoch73 loss:0.0494 using time is 56.28s\n",
            "[74,    50] loss: 0.001\n",
            "[74,   100] loss: 0.001\n",
            "[74,   150] loss: 0.001\n",
            "[74,   200] loss: 0.001\n",
            "[74,   250] loss: 0.001\n",
            "epoch74 loss:0.0926 using time is 56.18s\n",
            "[75,    50] loss: 0.001\n",
            "[75,   100] loss: 0.001\n",
            "[75,   150] loss: 0.001\n",
            "[75,   200] loss: 0.001\n",
            "[75,   250] loss: 0.001\n",
            "epoch75 loss:0.0652 using time is 56.28s\n",
            "[76,    50] loss: 0.001\n",
            "[76,   100] loss: 0.001\n",
            "[76,   150] loss: 0.001\n",
            "[76,   200] loss: 0.001\n",
            "[76,   250] loss: 0.001\n",
            "epoch76 loss:0.0136 using time is 56.11s\n",
            "[77,    50] loss: 0.001\n",
            "[77,   100] loss: 0.001\n",
            "[77,   150] loss: 0.001\n",
            "[77,   200] loss: 0.001\n",
            "[77,   250] loss: 0.001\n",
            "epoch77 loss:0.0355 using time is 56.42s\n",
            "[78,    50] loss: 0.001\n",
            "[78,   100] loss: 0.001\n",
            "[78,   150] loss: 0.001\n",
            "[78,   200] loss: 0.001\n",
            "[78,   250] loss: 0.001\n",
            "epoch78 loss:0.0698 using time is 56.29s\n",
            "[79,    50] loss: 0.001\n",
            "[79,   100] loss: 0.001\n",
            "[79,   150] loss: 0.001\n",
            "[79,   200] loss: 0.001\n",
            "[79,   250] loss: 0.001\n",
            "epoch79 loss:0.0203 using time is 56.25s\n",
            "[80,    50] loss: 0.001\n",
            "[80,   100] loss: 0.001\n",
            "[80,   150] loss: 0.001\n",
            "[80,   200] loss: 0.001\n",
            "[80,   250] loss: 0.001\n",
            "epoch80 loss:0.0225 using time is 56.27s\n",
            "[81,    50] loss: 0.001\n",
            "[81,   100] loss: 0.001\n",
            "[81,   150] loss: 0.001\n",
            "[81,   200] loss: 0.001\n",
            "[81,   250] loss: 0.001\n",
            "epoch81 loss:0.0312 using time is 56.24s\n",
            "[82,    50] loss: 0.001\n",
            "[82,   100] loss: 0.001\n",
            "[82,   150] loss: 0.001\n",
            "[82,   200] loss: 0.001\n",
            "[82,   250] loss: 0.001\n",
            "epoch82 loss:0.0337 using time is 56.28s\n",
            "[83,    50] loss: 0.001\n",
            "[83,   100] loss: 0.001\n",
            "[83,   150] loss: 0.001\n",
            "[83,   200] loss: 0.001\n",
            "[83,   250] loss: 0.001\n",
            "epoch83 loss:0.0247 using time is 56.08s\n",
            "[84,    50] loss: 0.001\n",
            "[84,   100] loss: 0.001\n",
            "[84,   150] loss: 0.001\n",
            "[84,   200] loss: 0.001\n",
            "[84,   250] loss: 0.001\n",
            "epoch84 loss:0.0300 using time is 56.12s\n",
            "[85,    50] loss: 0.001\n",
            "[85,   100] loss: 0.001\n",
            "[85,   150] loss: 0.001\n",
            "[85,   200] loss: 0.001\n",
            "[85,   250] loss: 0.001\n",
            "epoch85 loss:0.0293 using time is 56.21s\n",
            "[86,    50] loss: 0.001\n",
            "[86,   100] loss: 0.001\n",
            "[86,   150] loss: 0.001\n",
            "[86,   200] loss: 0.001\n",
            "[86,   250] loss: 0.001\n",
            "epoch86 loss:0.0314 using time is 56.07s\n",
            "[87,    50] loss: 0.001\n",
            "[87,   100] loss: 0.001\n",
            "[87,   150] loss: 0.001\n",
            "[87,   200] loss: 0.001\n",
            "[87,   250] loss: 0.001\n",
            "epoch87 loss:0.0229 using time is 56.24s\n",
            "[88,    50] loss: 0.001\n",
            "[88,   100] loss: 0.001\n",
            "[88,   150] loss: 0.001\n",
            "[88,   200] loss: 0.001\n",
            "[88,   250] loss: 0.001\n",
            "epoch88 loss:0.0275 using time is 56.06s\n",
            "[89,    50] loss: 0.001\n",
            "[89,   100] loss: 0.001\n",
            "[89,   150] loss: 0.001\n",
            "[89,   200] loss: 0.001\n",
            "[89,   250] loss: 0.001\n",
            "epoch89 loss:0.0138 using time is 56.13s\n",
            "[90,    50] loss: 0.001\n",
            "[90,   100] loss: 0.001\n",
            "[90,   150] loss: 0.001\n",
            "[90,   200] loss: 0.001\n",
            "[90,   250] loss: 0.001\n",
            "epoch90 loss:0.0189 using time is 56.22s\n",
            "[91,    50] loss: 0.001\n",
            "[91,   100] loss: 0.001\n",
            "[91,   150] loss: 0.001\n",
            "[91,   200] loss: 0.001\n",
            "[91,   250] loss: 0.001\n",
            "epoch91 loss:0.0150 using time is 56.39s\n",
            "[92,    50] loss: 0.001\n",
            "[92,   100] loss: 0.001\n",
            "[92,   150] loss: 0.001\n",
            "[92,   200] loss: 0.001\n",
            "[92,   250] loss: 0.001\n",
            "epoch92 loss:0.0347 using time is 55.93s\n",
            "[93,    50] loss: 0.001\n",
            "[93,   100] loss: 0.001\n",
            "[93,   150] loss: 0.001\n",
            "[93,   200] loss: 0.001\n",
            "[93,   250] loss: 0.001\n",
            "epoch93 loss:0.0040 using time is 56.19s\n",
            "[94,    50] loss: 0.001\n",
            "[94,   100] loss: 0.001\n",
            "[94,   150] loss: 0.001\n",
            "[94,   200] loss: 0.001\n",
            "[94,   250] loss: 0.001\n",
            "epoch94 loss:0.0124 using time is 56.08s\n",
            "[95,    50] loss: 0.001\n",
            "[95,   100] loss: 0.001\n",
            "[95,   150] loss: 0.001\n",
            "[95,   200] loss: 0.001\n",
            "[95,   250] loss: 0.001\n",
            "epoch95 loss:0.0277 using time is 55.86s\n",
            "[96,    50] loss: 0.000\n",
            "[96,   100] loss: 0.001\n",
            "[96,   150] loss: 0.001\n",
            "[96,   200] loss: 0.001\n",
            "[96,   250] loss: 0.001\n",
            "epoch96 loss:0.0476 using time is 55.98s\n",
            "[97,    50] loss: 0.001\n",
            "[97,   100] loss: 0.001\n",
            "[97,   150] loss: 0.001\n",
            "[97,   200] loss: 0.000\n",
            "[97,   250] loss: 0.001\n",
            "epoch97 loss:0.0235 using time is 56.01s\n",
            "[98,    50] loss: 0.001\n",
            "[98,   100] loss: 0.001\n",
            "[98,   150] loss: 0.001\n",
            "[98,   200] loss: 0.001\n",
            "[98,   250] loss: 0.001\n",
            "epoch98 loss:0.0256 using time is 56.13s\n",
            "[99,    50] loss: 0.001\n",
            "[99,   100] loss: 0.001\n",
            "[99,   150] loss: 0.001\n",
            "[99,   200] loss: 0.001\n",
            "[99,   250] loss: 0.001\n",
            "epoch99 loss:0.0342 using time is 56.01s\n",
            "[100,    50] loss: 0.001\n",
            "[100,   100] loss: 0.001\n",
            "[100,   150] loss: 0.001\n",
            "[100,   200] loss: 0.001\n",
            "[100,   250] loss: 0.001\n",
            "epoch100 loss:0.0344 using time is 56.13s\n",
            "[101,    50] loss: 0.001\n",
            "[101,   100] loss: 0.001\n",
            "[101,   150] loss: 0.001\n",
            "[101,   200] loss: 0.001\n",
            "[101,   250] loss: 0.001\n",
            "epoch101 loss:0.0110 using time is 55.74s\n",
            "[102,    50] loss: 0.000\n",
            "[102,   100] loss: 0.000\n",
            "[102,   150] loss: 0.001\n",
            "[102,   200] loss: 0.001\n",
            "[102,   250] loss: 0.001\n",
            "epoch102 loss:0.0150 using time is 55.92s\n",
            "[103,    50] loss: 0.001\n",
            "[103,   100] loss: 0.001\n",
            "[103,   150] loss: 0.001\n",
            "[103,   200] loss: 0.001\n",
            "[103,   250] loss: 0.001\n",
            "epoch103 loss:0.0087 using time is 56.10s\n",
            "[104,    50] loss: 0.001\n",
            "[104,   100] loss: 0.001\n",
            "[104,   150] loss: 0.001\n",
            "[104,   200] loss: 0.001\n",
            "[104,   250] loss: 0.001\n",
            "epoch104 loss:0.0245 using time is 56.15s\n",
            "[105,    50] loss: 0.001\n",
            "[105,   100] loss: 0.001\n",
            "[105,   150] loss: 0.001\n",
            "[105,   200] loss: 0.001\n",
            "[105,   250] loss: 0.001\n",
            "epoch105 loss:0.0240 using time is 55.97s\n",
            "[106,    50] loss: 0.001\n",
            "[106,   100] loss: 0.001\n",
            "[106,   150] loss: 0.001\n",
            "[106,   200] loss: 0.001\n",
            "[106,   250] loss: 0.001\n",
            "epoch106 loss:0.0401 using time is 56.08s\n",
            "[107,    50] loss: 0.001\n",
            "[107,   100] loss: 0.001\n",
            "[107,   150] loss: 0.001\n",
            "[107,   200] loss: 0.001\n",
            "[107,   250] loss: 0.001\n",
            "epoch107 loss:0.0192 using time is 55.89s\n",
            "[108,    50] loss: 0.001\n",
            "[108,   100] loss: 0.001\n",
            "[108,   150] loss: 0.001\n",
            "[108,   200] loss: 0.001\n",
            "[108,   250] loss: 0.001\n",
            "epoch108 loss:0.0166 using time is 56.11s\n",
            "[109,    50] loss: 0.001\n",
            "[109,   100] loss: 0.001\n",
            "[109,   150] loss: 0.000\n",
            "[109,   200] loss: 0.001\n",
            "[109,   250] loss: 0.001\n",
            "epoch109 loss:0.0218 using time is 56.03s\n",
            "[110,    50] loss: 0.001\n",
            "[110,   100] loss: 0.001\n",
            "[110,   150] loss: 0.001\n",
            "[110,   200] loss: 0.001\n",
            "[110,   250] loss: 0.001\n",
            "epoch110 loss:0.0324 using time is 55.96s\n",
            "[111,    50] loss: 0.001\n",
            "[111,   100] loss: 0.001\n",
            "[111,   150] loss: 0.001\n",
            "[111,   200] loss: 0.001\n",
            "[111,   250] loss: 0.000\n",
            "epoch111 loss:0.0135 using time is 55.88s\n",
            "[112,    50] loss: 0.001\n",
            "[112,   100] loss: 0.001\n",
            "[112,   150] loss: 0.001\n",
            "[112,   200] loss: 0.001\n",
            "[112,   250] loss: 0.001\n",
            "epoch112 loss:0.0389 using time is 55.88s\n",
            "[113,    50] loss: 0.001\n",
            "[113,   100] loss: 0.001\n",
            "[113,   150] loss: 0.001\n",
            "[113,   200] loss: 0.001\n",
            "[113,   250] loss: 0.000\n",
            "epoch113 loss:0.0319 using time is 55.88s\n",
            "[114,    50] loss: 0.001\n",
            "[114,   100] loss: 0.001\n",
            "[114,   150] loss: 0.001\n",
            "[114,   200] loss: 0.001\n",
            "[114,   250] loss: 0.001\n",
            "epoch114 loss:0.0293 using time is 55.97s\n",
            "[115,    50] loss: 0.000\n",
            "[115,   100] loss: 0.000\n",
            "[115,   150] loss: 0.000\n",
            "[115,   200] loss: 0.000\n",
            "[115,   250] loss: 0.001\n",
            "epoch115 loss:0.0274 using time is 55.93s\n",
            "[116,    50] loss: 0.001\n",
            "[116,   100] loss: 0.000\n",
            "[116,   150] loss: 0.001\n",
            "[116,   200] loss: 0.001\n",
            "[116,   250] loss: 0.001\n",
            "epoch116 loss:0.0317 using time is 55.87s\n",
            "[117,    50] loss: 0.000\n",
            "[117,   100] loss: 0.001\n",
            "[117,   150] loss: 0.001\n",
            "[117,   200] loss: 0.001\n",
            "[117,   250] loss: 0.000\n",
            "epoch117 loss:0.0105 using time is 55.97s\n",
            "[118,    50] loss: 0.001\n",
            "[118,   100] loss: 0.000\n",
            "[118,   150] loss: 0.000\n",
            "[118,   200] loss: 0.001\n",
            "[118,   250] loss: 0.001\n",
            "epoch118 loss:0.0129 using time is 55.82s\n",
            "[119,    50] loss: 0.001\n",
            "[119,   100] loss: 0.000\n",
            "[119,   150] loss: 0.001\n",
            "[119,   200] loss: 0.000\n",
            "[119,   250] loss: 0.001\n",
            "epoch119 loss:0.0072 using time is 55.67s\n",
            "[120,    50] loss: 0.000\n",
            "[120,   100] loss: 0.001\n",
            "[120,   150] loss: 0.001\n",
            "[120,   200] loss: 0.000\n",
            "[120,   250] loss: 0.001\n",
            "epoch120 loss:0.0141 using time is 55.60s\n",
            "[121,    50] loss: 0.000\n",
            "[121,   100] loss: 0.000\n",
            "[121,   150] loss: 0.001\n",
            "[121,   200] loss: 0.001\n",
            "[121,   250] loss: 0.001\n",
            "epoch121 loss:0.0084 using time is 55.83s\n",
            "[122,    50] loss: 0.000\n",
            "[122,   100] loss: 0.000\n",
            "[122,   150] loss: 0.000\n",
            "[122,   200] loss: 0.001\n",
            "[122,   250] loss: 0.001\n",
            "epoch122 loss:0.0143 using time is 55.79s\n",
            "[123,    50] loss: 0.000\n",
            "[123,   100] loss: 0.001\n",
            "[123,   150] loss: 0.001\n",
            "[123,   200] loss: 0.000\n",
            "[123,   250] loss: 0.001\n",
            "epoch123 loss:0.0105 using time is 55.78s\n",
            "[124,    50] loss: 0.001\n",
            "[124,   100] loss: 0.001\n",
            "[124,   150] loss: 0.000\n",
            "[124,   200] loss: 0.001\n",
            "[124,   250] loss: 0.001\n",
            "epoch124 loss:0.0450 using time is 55.55s\n",
            "[125,    50] loss: 0.000\n",
            "[125,   100] loss: 0.001\n",
            "[125,   150] loss: 0.000\n",
            "[125,   200] loss: 0.001\n",
            "[125,   250] loss: 0.001\n",
            "epoch125 loss:0.0153 using time is 55.55s\n",
            "[126,    50] loss: 0.001\n",
            "[126,   100] loss: 0.000\n",
            "[126,   150] loss: 0.000\n",
            "[126,   200] loss: 0.001\n",
            "[126,   250] loss: 0.000\n",
            "epoch126 loss:0.0130 using time is 55.55s\n",
            "[127,    50] loss: 0.000\n",
            "[127,   100] loss: 0.000\n",
            "[127,   150] loss: 0.000\n",
            "[127,   200] loss: 0.000\n",
            "[127,   250] loss: 0.001\n",
            "epoch127 loss:0.0049 using time is 55.58s\n",
            "[128,    50] loss: 0.001\n",
            "[128,   100] loss: 0.001\n",
            "[128,   150] loss: 0.001\n",
            "[128,   200] loss: 0.001\n",
            "[128,   250] loss: 0.001\n",
            "epoch128 loss:0.0450 using time is 55.63s\n",
            "[129,    50] loss: 0.000\n",
            "[129,   100] loss: 0.000\n",
            "[129,   150] loss: 0.000\n",
            "[129,   200] loss: 0.001\n",
            "[129,   250] loss: 0.001\n",
            "epoch129 loss:0.0033 using time is 55.53s\n",
            "[130,    50] loss: 0.000\n",
            "[130,   100] loss: 0.000\n",
            "[130,   150] loss: 0.001\n",
            "[130,   200] loss: 0.000\n",
            "[130,   250] loss: 0.000\n",
            "epoch130 loss:0.0379 using time is 55.68s\n",
            "[131,    50] loss: 0.000\n",
            "[131,   100] loss: 0.000\n",
            "[131,   150] loss: 0.000\n",
            "[131,   200] loss: 0.001\n",
            "[131,   250] loss: 0.000\n",
            "epoch131 loss:0.0195 using time is 55.70s\n",
            "[132,    50] loss: 0.000\n",
            "[132,   100] loss: 0.000\n",
            "[132,   150] loss: 0.001\n",
            "[132,   200] loss: 0.001\n",
            "[132,   250] loss: 0.001\n",
            "epoch132 loss:0.0377 using time is 55.65s\n",
            "[133,    50] loss: 0.000\n",
            "[133,   100] loss: 0.000\n",
            "[133,   150] loss: 0.000\n",
            "[133,   200] loss: 0.000\n",
            "[133,   250] loss: 0.001\n",
            "epoch133 loss:0.0198 using time is 55.84s\n",
            "[134,    50] loss: 0.000\n",
            "[134,   100] loss: 0.000\n",
            "[134,   150] loss: 0.001\n",
            "[134,   200] loss: 0.001\n",
            "[134,   250] loss: 0.001\n",
            "epoch134 loss:0.0058 using time is 55.59s\n",
            "[135,    50] loss: 0.000\n",
            "[135,   100] loss: 0.000\n",
            "[135,   150] loss: 0.000\n",
            "[135,   200] loss: 0.001\n",
            "[135,   250] loss: 0.001\n",
            "epoch135 loss:0.0069 using time is 55.43s\n",
            "[136,    50] loss: 0.000\n",
            "[136,   100] loss: 0.000\n",
            "[136,   150] loss: 0.000\n",
            "[136,   200] loss: 0.000\n",
            "[136,   250] loss: 0.000\n",
            "epoch136 loss:0.0385 using time is 55.57s\n",
            "[137,    50] loss: 0.000\n",
            "[137,   100] loss: 0.000\n",
            "[137,   150] loss: 0.001\n",
            "[137,   200] loss: 0.001\n",
            "[137,   250] loss: 0.000\n",
            "epoch137 loss:0.0144 using time is 55.75s\n",
            "[138,    50] loss: 0.000\n",
            "[138,   100] loss: 0.000\n",
            "[138,   150] loss: 0.001\n",
            "[138,   200] loss: 0.000\n",
            "[138,   250] loss: 0.000\n",
            "epoch138 loss:0.0132 using time is 55.52s\n",
            "[139,    50] loss: 0.001\n",
            "[139,   100] loss: 0.000\n",
            "[139,   150] loss: 0.000\n",
            "[139,   200] loss: 0.000\n",
            "[139,   250] loss: 0.000\n",
            "epoch139 loss:0.0047 using time is 55.58s\n",
            "[140,    50] loss: 0.000\n",
            "[140,   100] loss: 0.000\n",
            "[140,   150] loss: 0.000\n",
            "[140,   200] loss: 0.001\n",
            "[140,   250] loss: 0.000\n",
            "epoch140 loss:0.0028 using time is 55.71s\n",
            "[141,    50] loss: 0.000\n",
            "[141,   100] loss: 0.000\n",
            "[141,   150] loss: 0.000\n",
            "[141,   200] loss: 0.000\n",
            "[141,   250] loss: 0.000\n",
            "epoch141 loss:0.0152 using time is 55.52s\n",
            "[142,    50] loss: 0.000\n",
            "[142,   100] loss: 0.000\n",
            "[142,   150] loss: 0.000\n",
            "[142,   200] loss: 0.000\n",
            "[142,   250] loss: 0.001\n",
            "epoch142 loss:0.0103 using time is 56.49s\n",
            "[143,    50] loss: 0.000\n",
            "[143,   100] loss: 0.000\n",
            "[143,   150] loss: 0.000\n",
            "[143,   200] loss: 0.000\n",
            "[143,   250] loss: 0.001\n",
            "epoch143 loss:0.0288 using time is 55.71s\n",
            "[144,    50] loss: 0.000\n",
            "[144,   100] loss: 0.000\n",
            "[144,   150] loss: 0.000\n",
            "[144,   200] loss: 0.001\n",
            "[144,   250] loss: 0.001\n",
            "epoch144 loss:0.0631 using time is 55.80s\n",
            "[145,    50] loss: 0.000\n",
            "[145,   100] loss: 0.000\n",
            "[145,   150] loss: 0.000\n",
            "[145,   200] loss: 0.001\n",
            "[145,   250] loss: 0.000\n",
            "epoch145 loss:0.0141 using time is 55.84s\n",
            "[146,    50] loss: 0.000\n",
            "[146,   100] loss: 0.000\n",
            "[146,   150] loss: 0.000\n",
            "[146,   200] loss: 0.001\n",
            "[146,   250] loss: 0.000\n",
            "epoch146 loss:0.0068 using time is 55.77s\n",
            "[147,    50] loss: 0.000\n",
            "[147,   100] loss: 0.000\n",
            "[147,   150] loss: 0.000\n",
            "[147,   200] loss: 0.000\n",
            "[147,   250] loss: 0.000\n",
            "epoch147 loss:0.0045 using time is 55.59s\n",
            "[148,    50] loss: 0.000\n",
            "[148,   100] loss: 0.000\n",
            "[148,   150] loss: 0.000\n",
            "[148,   200] loss: 0.000\n",
            "[148,   250] loss: 0.000\n",
            "epoch148 loss:0.0027 using time is 55.44s\n",
            "[149,    50] loss: 0.000\n",
            "[149,   100] loss: 0.000\n",
            "[149,   150] loss: 0.000\n",
            "[149,   200] loss: 0.000\n",
            "[149,   250] loss: 0.000\n",
            "epoch149 loss:0.0024 using time is 55.49s\n",
            "[150,    50] loss: 0.000\n",
            "[150,   100] loss: 0.000\n",
            "[150,   150] loss: 0.000\n",
            "[150,   200] loss: 0.000\n",
            "[150,   250] loss: 0.000\n",
            "epoch150 loss:0.0050 using time is 55.68s\n",
            "[151,    50] loss: 0.000\n",
            "[151,   100] loss: 0.000\n",
            "[151,   150] loss: 0.000\n",
            "[151,   200] loss: 0.000\n",
            "[151,   250] loss: 0.000\n",
            "epoch151 loss:0.0061 using time is 55.74s\n",
            "[152,    50] loss: 0.000\n",
            "[152,   100] loss: 0.000\n",
            "[152,   150] loss: 0.000\n",
            "[152,   200] loss: 0.000\n",
            "[152,   250] loss: 0.000\n",
            "epoch152 loss:0.0191 using time is 55.51s\n",
            "[153,    50] loss: 0.000\n",
            "[153,   100] loss: 0.000\n",
            "[153,   150] loss: 0.000\n",
            "[153,   200] loss: 0.000\n",
            "[153,   250] loss: 0.000\n",
            "epoch153 loss:0.0106 using time is 55.54s\n",
            "[154,    50] loss: 0.000\n",
            "[154,   100] loss: 0.000\n",
            "[154,   150] loss: 0.001\n",
            "[154,   200] loss: 0.001\n",
            "[154,   250] loss: 0.001\n",
            "epoch154 loss:0.0084 using time is 55.50s\n",
            "[155,    50] loss: 0.001\n",
            "[155,   100] loss: 0.000\n",
            "[155,   150] loss: 0.000\n",
            "[155,   200] loss: 0.000\n",
            "[155,   250] loss: 0.001\n",
            "epoch155 loss:0.0212 using time is 55.59s\n",
            "[156,    50] loss: 0.000\n",
            "[156,   100] loss: 0.000\n",
            "[156,   150] loss: 0.001\n",
            "[156,   200] loss: 0.000\n",
            "[156,   250] loss: 0.000\n",
            "epoch156 loss:0.0200 using time is 55.73s\n",
            "[157,    50] loss: 0.000\n",
            "[157,   100] loss: 0.000\n",
            "[157,   150] loss: 0.000\n",
            "[157,   200] loss: 0.000\n",
            "[157,   250] loss: 0.000\n",
            "epoch157 loss:0.0154 using time is 55.60s\n",
            "[158,    50] loss: 0.000\n",
            "[158,   100] loss: 0.000\n",
            "[158,   150] loss: 0.000\n",
            "[158,   200] loss: 0.000\n",
            "[158,   250] loss: 0.000\n",
            "epoch158 loss:0.0255 using time is 55.54s\n",
            "[159,    50] loss: 0.000\n",
            "[159,   100] loss: 0.000\n",
            "[159,   150] loss: 0.000\n",
            "[159,   200] loss: 0.000\n",
            "[159,   250] loss: 0.000\n",
            "epoch159 loss:0.0087 using time is 55.62s\n",
            "[160,    50] loss: 0.000\n",
            "[160,   100] loss: 0.000\n",
            "[160,   150] loss: 0.000\n",
            "[160,   200] loss: 0.000\n",
            "[160,   250] loss: 0.001\n",
            "epoch160 loss:0.0049 using time is 55.63s\n",
            "[161,    50] loss: 0.000\n",
            "[161,   100] loss: 0.000\n",
            "[161,   150] loss: 0.000\n",
            "[161,   200] loss: 0.000\n",
            "[161,   250] loss: 0.001\n",
            "epoch161 loss:0.0380 using time is 55.73s\n",
            "[162,    50] loss: 0.000\n",
            "[162,   100] loss: 0.000\n",
            "[162,   150] loss: 0.000\n",
            "[162,   200] loss: 0.000\n",
            "[162,   250] loss: 0.000\n",
            "epoch162 loss:0.0093 using time is 55.82s\n",
            "[163,    50] loss: 0.000\n",
            "[163,   100] loss: 0.000\n",
            "[163,   150] loss: 0.000\n",
            "[163,   200] loss: 0.000\n",
            "[163,   250] loss: 0.000\n",
            "epoch163 loss:0.0036 using time is 55.55s\n",
            "[164,    50] loss: 0.000\n",
            "[164,   100] loss: 0.000\n",
            "[164,   150] loss: 0.000\n",
            "[164,   200] loss: 0.000\n",
            "[164,   250] loss: 0.000\n",
            "epoch164 loss:0.0103 using time is 55.63s\n",
            "[165,    50] loss: 0.000\n",
            "[165,   100] loss: 0.000\n",
            "[165,   150] loss: 0.000\n",
            "[165,   200] loss: 0.000\n",
            "[165,   250] loss: 0.000\n",
            "epoch165 loss:0.0259 using time is 55.77s\n",
            "[166,    50] loss: 0.000\n",
            "[166,   100] loss: 0.000\n",
            "[166,   150] loss: 0.000\n",
            "[166,   200] loss: 0.000\n",
            "[166,   250] loss: 0.000\n",
            "epoch166 loss:0.0018 using time is 55.81s\n",
            "[167,    50] loss: 0.000\n",
            "[167,   100] loss: 0.000\n",
            "[167,   150] loss: 0.000\n",
            "[167,   200] loss: 0.000\n",
            "[167,   250] loss: 0.000\n",
            "epoch167 loss:0.0210 using time is 55.95s\n",
            "[168,    50] loss: 0.000\n",
            "[168,   100] loss: 0.000\n",
            "[168,   150] loss: 0.000\n",
            "[168,   200] loss: 0.000\n",
            "[168,   250] loss: 0.001\n",
            "epoch168 loss:0.0307 using time is 55.88s\n",
            "[169,    50] loss: 0.001\n",
            "[169,   100] loss: 0.001\n",
            "[169,   150] loss: 0.001\n",
            "[169,   200] loss: 0.001\n",
            "[169,   250] loss: 0.001\n",
            "epoch169 loss:0.0133 using time is 55.70s\n",
            "[170,    50] loss: 0.000\n",
            "[170,   100] loss: 0.000\n",
            "[170,   150] loss: 0.000\n",
            "[170,   200] loss: 0.000\n",
            "[170,   250] loss: 0.000\n",
            "epoch170 loss:0.0041 using time is 55.83s\n",
            "[171,    50] loss: 0.000\n",
            "[171,   100] loss: 0.000\n",
            "[171,   150] loss: 0.000\n",
            "[171,   200] loss: 0.000\n",
            "[171,   250] loss: 0.000\n",
            "epoch171 loss:0.0301 using time is 55.75s\n",
            "[172,    50] loss: 0.000\n",
            "[172,   100] loss: 0.000\n",
            "[172,   150] loss: 0.000\n",
            "[172,   200] loss: 0.000\n",
            "[172,   250] loss: 0.000\n",
            "epoch172 loss:0.0146 using time is 55.67s\n",
            "[173,    50] loss: 0.000\n",
            "[173,   100] loss: 0.000\n",
            "[173,   150] loss: 0.000\n",
            "[173,   200] loss: 0.000\n",
            "[173,   250] loss: 0.000\n",
            "epoch173 loss:0.0082 using time is 55.69s\n",
            "[174,    50] loss: 0.000\n",
            "[174,   100] loss: 0.000\n",
            "[174,   150] loss: 0.000\n",
            "[174,   200] loss: 0.000\n",
            "[174,   250] loss: 0.000\n",
            "epoch174 loss:0.0166 using time is 55.52s\n",
            "[175,    50] loss: 0.000\n",
            "[175,   100] loss: 0.000\n",
            "[175,   150] loss: 0.000\n",
            "[175,   200] loss: 0.000\n",
            "[175,   250] loss: 0.000\n",
            "epoch175 loss:0.0150 using time is 55.80s\n",
            "[176,    50] loss: 0.000\n",
            "[176,   100] loss: 0.000\n",
            "[176,   150] loss: 0.000\n",
            "[176,   200] loss: 0.000\n",
            "[176,   250] loss: 0.000\n",
            "epoch176 loss:0.0032 using time is 55.51s\n",
            "[177,    50] loss: 0.000\n",
            "[177,   100] loss: 0.000\n",
            "[177,   150] loss: 0.000\n",
            "[177,   200] loss: 0.000\n",
            "[177,   250] loss: 0.000\n",
            "epoch177 loss:0.0200 using time is 55.47s\n",
            "[178,    50] loss: 0.000\n",
            "[178,   100] loss: 0.000\n",
            "[178,   150] loss: 0.000\n",
            "[178,   200] loss: 0.000\n",
            "[178,   250] loss: 0.000\n",
            "epoch178 loss:0.0215 using time is 55.80s\n",
            "[179,    50] loss: 0.000\n",
            "[179,   100] loss: 0.000\n",
            "[179,   150] loss: 0.000\n",
            "[179,   200] loss: 0.000\n",
            "[179,   250] loss: 0.000\n",
            "epoch179 loss:0.0067 using time is 55.80s\n",
            "[180,    50] loss: 0.000\n",
            "[180,   100] loss: 0.000\n",
            "[180,   150] loss: 0.000\n",
            "[180,   200] loss: 0.000\n",
            "[180,   250] loss: 0.000\n",
            "epoch180 loss:0.0171 using time is 55.82s\n",
            "[181,    50] loss: 0.000\n",
            "[181,   100] loss: 0.000\n",
            "[181,   150] loss: 0.000\n",
            "[181,   200] loss: 0.000\n",
            "[181,   250] loss: 0.000\n",
            "epoch181 loss:0.0123 using time is 55.71s\n",
            "[182,    50] loss: 0.000\n",
            "[182,   100] loss: 0.000\n",
            "[182,   150] loss: 0.000\n",
            "[182,   200] loss: 0.000\n",
            "[182,   250] loss: 0.000\n",
            "epoch182 loss:0.0140 using time is 55.75s\n",
            "[183,    50] loss: 0.000\n",
            "[183,   100] loss: 0.000\n",
            "[183,   150] loss: 0.000\n",
            "[183,   200] loss: 0.000\n",
            "[183,   250] loss: 0.000\n",
            "epoch183 loss:0.0191 using time is 55.69s\n",
            "[184,    50] loss: 0.000\n",
            "[184,   100] loss: 0.000\n",
            "[184,   150] loss: 0.000\n",
            "[184,   200] loss: 0.000\n",
            "[184,   250] loss: 0.001\n",
            "epoch184 loss:0.0028 using time is 55.87s\n",
            "[185,    50] loss: 0.000\n",
            "[185,   100] loss: 0.000\n",
            "[185,   150] loss: 0.000\n",
            "[185,   200] loss: 0.000\n",
            "[185,   250] loss: 0.000\n",
            "epoch185 loss:0.0136 using time is 55.54s\n",
            "[186,    50] loss: 0.000\n",
            "[186,   100] loss: 0.000\n",
            "[186,   150] loss: 0.000\n",
            "[186,   200] loss: 0.000\n",
            "[186,   250] loss: 0.000\n",
            "epoch186 loss:0.0040 using time is 55.69s\n",
            "[187,    50] loss: 0.000\n",
            "[187,   100] loss: 0.000\n",
            "[187,   150] loss: 0.000\n",
            "[187,   200] loss: 0.000\n",
            "[187,   250] loss: 0.000\n",
            "epoch187 loss:0.0182 using time is 55.78s\n",
            "[188,    50] loss: 0.000\n",
            "[188,   100] loss: 0.000\n",
            "[188,   150] loss: 0.000\n",
            "[188,   200] loss: 0.000\n",
            "[188,   250] loss: 0.000\n",
            "epoch188 loss:0.0106 using time is 55.90s\n",
            "[189,    50] loss: 0.000\n",
            "[189,   100] loss: 0.000\n",
            "[189,   150] loss: 0.000\n",
            "[189,   200] loss: 0.000\n",
            "[189,   250] loss: 0.000\n",
            "epoch189 loss:0.0262 using time is 55.75s\n",
            "[190,    50] loss: 0.000\n",
            "[190,   100] loss: 0.000\n",
            "[190,   150] loss: 0.000\n",
            "[190,   200] loss: 0.000\n",
            "[190,   250] loss: 0.000\n",
            "epoch190 loss:0.0060 using time is 55.78s\n",
            "[191,    50] loss: 0.000\n",
            "[191,   100] loss: 0.000\n",
            "[191,   150] loss: 0.000\n",
            "[191,   200] loss: 0.000\n",
            "[191,   250] loss: 0.000\n",
            "epoch191 loss:0.0059 using time is 55.96s\n",
            "[192,    50] loss: 0.000\n",
            "[192,   100] loss: 0.000\n",
            "[192,   150] loss: 0.000\n",
            "[192,   200] loss: 0.000\n",
            "[192,   250] loss: 0.000\n",
            "epoch192 loss:0.0101 using time is 55.85s\n",
            "[193,    50] loss: 0.000\n",
            "[193,   100] loss: 0.000\n",
            "[193,   150] loss: 0.000\n",
            "[193,   200] loss: 0.000\n",
            "[193,   250] loss: 0.000\n",
            "epoch193 loss:0.0078 using time is 55.75s\n",
            "[194,    50] loss: 0.000\n",
            "[194,   100] loss: 0.000\n",
            "[194,   150] loss: 0.000\n",
            "[194,   200] loss: 0.000\n",
            "[194,   250] loss: 0.000\n",
            "epoch194 loss:0.0151 using time is 55.88s\n",
            "[195,    50] loss: 0.000\n",
            "[195,   100] loss: 0.000\n",
            "[195,   150] loss: 0.000\n",
            "[195,   200] loss: 0.000\n",
            "[195,   250] loss: 0.000\n",
            "epoch195 loss:0.0051 using time is 55.96s\n",
            "[196,    50] loss: 0.000\n",
            "[196,   100] loss: 0.000\n",
            "[196,   150] loss: 0.000\n",
            "[196,   200] loss: 0.000\n",
            "[196,   250] loss: 0.000\n",
            "epoch196 loss:0.0055 using time is 56.02s\n",
            "[197,    50] loss: 0.000\n",
            "[197,   100] loss: 0.000\n",
            "[197,   150] loss: 0.000\n",
            "[197,   200] loss: 0.000\n",
            "[197,   250] loss: 0.000\n",
            "epoch197 loss:0.0217 using time is 56.00s\n",
            "[198,    50] loss: 0.000\n",
            "[198,   100] loss: 0.000\n",
            "[198,   150] loss: 0.000\n",
            "[198,   200] loss: 0.000\n",
            "[198,   250] loss: 0.000\n",
            "epoch198 loss:0.0078 using time is 55.93s\n",
            "[199,    50] loss: 0.000\n",
            "[199,   100] loss: 0.000\n",
            "[199,   150] loss: 0.000\n",
            "[199,   200] loss: 0.000\n",
            "[199,   250] loss: 0.000\n",
            "epoch199 loss:0.0039 using time is 55.79s\n",
            "[200,    50] loss: 0.000\n",
            "[200,   100] loss: 0.000\n",
            "[200,   150] loss: 0.000\n",
            "[200,   200] loss: 0.000\n",
            "[200,   250] loss: 0.000\n",
            "epoch200 loss:0.0114 using time is 55.97s\n",
            "[201,    50] loss: 0.000\n",
            "[201,   100] loss: 0.000\n",
            "[201,   150] loss: 0.000\n",
            "[201,   200] loss: 0.000\n",
            "[201,   250] loss: 0.000\n",
            "epoch201 loss:0.0036 using time is 56.12s\n",
            "[202,    50] loss: 0.000\n",
            "[202,   100] loss: 0.000\n",
            "[202,   150] loss: 0.000\n",
            "[202,   200] loss: 0.000\n",
            "[202,   250] loss: 0.000\n",
            "epoch202 loss:0.0037 using time is 55.96s\n",
            "[203,    50] loss: 0.000\n",
            "[203,   100] loss: 0.000\n",
            "[203,   150] loss: 0.000\n",
            "[203,   200] loss: 0.000\n",
            "[203,   250] loss: 0.001\n",
            "epoch203 loss:0.0012 using time is 55.83s\n",
            "[204,    50] loss: 0.000\n",
            "[204,   100] loss: 0.000\n",
            "[204,   150] loss: 0.000\n",
            "[204,   200] loss: 0.000\n",
            "[204,   250] loss: 0.000\n",
            "epoch204 loss:0.0019 using time is 55.63s\n",
            "[205,    50] loss: 0.000\n",
            "[205,   100] loss: 0.000\n",
            "[205,   150] loss: 0.000\n",
            "[205,   200] loss: 0.000\n",
            "[205,   250] loss: 0.000\n",
            "epoch205 loss:0.0049 using time is 56.03s\n",
            "[206,    50] loss: 0.000\n",
            "[206,   100] loss: 0.000\n",
            "[206,   150] loss: 0.000\n",
            "[206,   200] loss: 0.000\n",
            "[206,   250] loss: 0.000\n",
            "epoch206 loss:0.0147 using time is 55.81s\n",
            "[207,    50] loss: 0.000\n",
            "[207,   100] loss: 0.000\n",
            "[207,   150] loss: 0.000\n",
            "[207,   200] loss: 0.000\n",
            "[207,   250] loss: 0.000\n",
            "epoch207 loss:0.0090 using time is 55.97s\n",
            "[208,    50] loss: 0.000\n",
            "[208,   100] loss: 0.000\n",
            "[208,   150] loss: 0.000\n",
            "[208,   200] loss: 0.000\n",
            "[208,   250] loss: 0.000\n",
            "epoch208 loss:0.0176 using time is 56.02s\n",
            "[209,    50] loss: 0.000\n",
            "[209,   100] loss: 0.000\n",
            "[209,   150] loss: 0.000\n",
            "[209,   200] loss: 0.000\n",
            "[209,   250] loss: 0.000\n",
            "epoch209 loss:0.0048 using time is 55.95s\n",
            "[210,    50] loss: 0.000\n",
            "[210,   100] loss: 0.000\n",
            "[210,   150] loss: 0.000\n",
            "[210,   200] loss: 0.000\n",
            "[210,   250] loss: 0.000\n",
            "epoch210 loss:0.0407 using time is 56.00s\n",
            "[211,    50] loss: 0.000\n",
            "[211,   100] loss: 0.000\n",
            "[211,   150] loss: 0.000\n",
            "[211,   200] loss: 0.000\n",
            "[211,   250] loss: 0.000\n",
            "epoch211 loss:0.0093 using time is 55.84s\n",
            "[212,    50] loss: 0.000\n",
            "[212,   100] loss: 0.000\n",
            "[212,   150] loss: 0.000\n",
            "[212,   200] loss: 0.000\n",
            "[212,   250] loss: 0.000\n",
            "epoch212 loss:0.0067 using time is 56.02s\n",
            "[213,    50] loss: 0.000\n",
            "[213,   100] loss: 0.000\n",
            "[213,   150] loss: 0.000\n",
            "[213,   200] loss: 0.000\n",
            "[213,   250] loss: 0.000\n",
            "epoch213 loss:0.0032 using time is 56.09s\n",
            "[214,    50] loss: 0.000\n",
            "[214,   100] loss: 0.000\n",
            "[214,   150] loss: 0.000\n",
            "[214,   200] loss: 0.000\n",
            "[214,   250] loss: 0.000\n",
            "epoch214 loss:0.0290 using time is 56.14s\n",
            "[215,    50] loss: 0.000\n",
            "[215,   100] loss: 0.000\n",
            "[215,   150] loss: 0.000\n",
            "[215,   200] loss: 0.000\n",
            "[215,   250] loss: 0.000\n",
            "epoch215 loss:0.0044 using time is 56.12s\n",
            "[216,    50] loss: 0.000\n",
            "[216,   100] loss: 0.000\n",
            "[216,   150] loss: 0.000\n",
            "[216,   200] loss: 0.000\n",
            "[216,   250] loss: 0.000\n",
            "epoch216 loss:0.0130 using time is 56.04s\n",
            "[217,    50] loss: 0.000\n",
            "[217,   100] loss: 0.000\n",
            "[217,   150] loss: 0.000\n",
            "[217,   200] loss: 0.000\n",
            "[217,   250] loss: 0.000\n",
            "epoch217 loss:0.0086 using time is 56.18s\n",
            "[218,    50] loss: 0.000\n",
            "[218,   100] loss: 0.000\n",
            "[218,   150] loss: 0.000\n",
            "[218,   200] loss: 0.000\n",
            "[218,   250] loss: 0.000\n",
            "epoch218 loss:0.0064 using time is 56.08s\n",
            "[219,    50] loss: 0.000\n",
            "[219,   100] loss: 0.000\n",
            "[219,   150] loss: 0.001\n",
            "[219,   200] loss: 0.001\n",
            "[219,   250] loss: 0.001\n",
            "epoch219 loss:0.0296 using time is 56.07s\n",
            "[220,    50] loss: 0.001\n",
            "[220,   100] loss: 0.001\n",
            "[220,   150] loss: 0.001\n",
            "[220,   200] loss: 0.000\n",
            "[220,   250] loss: 0.000\n",
            "epoch220 loss:0.0328 using time is 56.12s\n",
            "[221,    50] loss: 0.000\n",
            "[221,   100] loss: 0.000\n",
            "[221,   150] loss: 0.000\n",
            "[221,   200] loss: 0.000\n",
            "[221,   250] loss: 0.000\n",
            "epoch221 loss:0.0054 using time is 56.09s\n",
            "[222,    50] loss: 0.000\n",
            "[222,   100] loss: 0.000\n",
            "[222,   150] loss: 0.000\n",
            "[222,   200] loss: 0.000\n",
            "[222,   250] loss: 0.000\n",
            "epoch222 loss:0.0009 using time is 56.00s\n",
            "Training Stop\n",
            "Finished Traning\n",
            "10000测试图像 准确率:87.5300%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7w6vX7t0qlA",
        "outputId": "12eec4a7-9971-44c9-d58b-2099101dea70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "pip install pytorchtools"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorchtools\n",
            "  Downloading https://files.pythonhosted.org/packages/07/7d/91fc993eed451ff2aff02e09a8e0246125b72cbc7068cb94242ca2c0e72f/pytorchtools-0.0.2-py2.py3-none-any.whl\n",
            "Installing collected packages: pytorchtools\n",
            "Successfully installed pytorchtools-0.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Soq682SLh9uy"
      },
      "source": [
        "# RESNET101的方式对CIFAR10进行分类预测（Early Stopping）"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTNU10ntiA26",
        "outputId": "8a55487a-e1bf-4083-dd44-7e09a5206cdc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.transforms import ToPILImage\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import pytorchtools\n",
        "%matplotlib inline\n",
        "import time\n",
        "import numpy as np\n",
        "show = ToPILImage() #可以把Tensor转换成Image,方便可视化\n",
        "transform = transforms.Compose([  #transforms.Compose就是将对图像处理的方法集中起来\n",
        "    transforms.RandomHorizontalFlip(),#水平翻转\n",
        "    transforms.RandomCrop((32, 32), padding=4),#对图片进行随机裁剪，裁剪的大小是32*32的，填充是4\n",
        "#     参数：size- (sequence or int)，若为sequence,则为(h,w)，若为int，则(size,size)\n",
        "#     padding-(sequence or int, optional)，此参数是设置填充多少个pixel。\n",
        "#     当为int时，图像上下左右均填充int个，例如padding=4，则上下左右均填充4个pixel，若为32x32，则会变成40x40。\n",
        "#     当为sequence时，若有2个数，则第一个数表示左右扩充多少，第二个数表示上下的。当有4个数时，则为左，上，右，下。\n",
        "#     fill- (int or tuple) 填充的值是什么（仅当填充模式为constant时有用）。int时，各通道均填充该值，当长度为3的tuple时，表示RGB通道需要填充的值。\n",
        "    transforms.ToTensor(),#转为Tensor\n",
        "    #在做数据归一化之前必须要把PIL Image转成Tensor，而其他resize或crop操作则不需要。\n",
        "     transforms.Normalize((0.5, 0.5 ,0.5), (0.5, 0.5, 0.5)),#归一化\n",
        "#     归一化操作，这里有两个参数一个是均值，一个是方差，由于是RGB型的所以一个参有三个值，这里面的参数的大小是可调的，调节的公式是：\n",
        "#     class torchvision.transforms.Normalize(mean, std)，\n",
        "#     Normalized_image=(image-mean)/std\n",
        "#     channel=（channel-mean）/std(因为transforms.ToTensor()已经把数据处理成[0,1],那么(x-0.5)/0.5\n",
        "#     就是[-1.0, 1.0])这样一来，我们的数据中的每个值就变成了[-1,1]的数了。\n",
        "    ])\n",
        "\n",
        "# CIFAR-10数据集下载\n",
        "train_data = datasets.CIFAR10(root='/content/drive/My Drive/Colab Notebooks/datas/datasets_data',\n",
        "                         train=True,                         # 训练集\n",
        "                         # 数据变换(0, 255) -> (0, 1)\n",
        "                         transform=transform   \n",
        "                           )\n",
        "\n",
        "test_data = datasets.CIFAR10(root='/content/drive/My Drive/Colab Notebooks/datas/datasets_data',\n",
        "                        train=False,                         # 测试集\n",
        "                        transform=transform\n",
        "                        #download=True\n",
        "                          )\n",
        "\n",
        "#划分训练集用于生成验证集\n",
        "valid_size = 0.2\n",
        "num_train = len(train_data)\n",
        "indices = list(range(num_train))\n",
        "np.random.shuffle(indices)\n",
        "split = int(np.floor(valid_size * num_train))\n",
        "train_idx, valid_idx = indices[split:], indices[:split]\n",
        "\n",
        "#定义用于获取训练和验证批次的采样器\n",
        "train_sampler = SubsetRandomSampler(train_idx)\n",
        "valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "# 使用DataLoader进行分批\n",
        "train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, sampler=train_sampler, num_workers=0)\n",
        "valid_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, sampler=valid_sampler, num_workers=0)\n",
        "test_loader = DataLoader(dataset=test_data, batch_size=BATCH_SIZE)\n",
        "\n",
        "#数据集10个类的定义\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "\n",
        "# from pytorchtools import EarlyStopping\n",
        "class EarlyStopping:\n",
        "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
        "    def __init__(self, patience=7, verbose=False, delta=0):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How long to wait after last time validation loss improved.\n",
        "                            Default: 7\n",
        "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
        "                            Default: False\n",
        "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
        "                            Default: 0\n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.delta = delta\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''Saves model when validation loss decrease.'''\n",
        "        if self.verbose:\n",
        "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        torch.save(model.state_dict(), 'checkpoint.pt')\t# 这里会存储迄今最优模型的参数\n",
        "        self.val_loss_min = val_loss\n",
        "\n",
        "# resnet101 Model\n",
        "model = torchvision.models.resnet101(pretrained=True)\n",
        "\n",
        "batch_size = 256\n",
        "n_epochs = 200\n",
        "\n",
        "# early stopping patience; how long to wait after last time validation loss improved.\n",
        "patience = 20\n",
        "    \n",
        "#损失函数:这里用交叉熵\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "#优化器 Adam\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "#device : GPU or CPU\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# to track the training loss as the model trains\n",
        "train_losses = []\n",
        "# to track the validation loss as the model trains\n",
        "valid_losses = []\n",
        "# to track the average training loss per epoch as the model trains\n",
        "avg_train_losses = []\n",
        "# to track the average validation loss per epoch as the model trains\n",
        "avg_valid_losses = [] \n",
        "\n",
        "# initialize the early_stopping object\n",
        "early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
        "\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "\n",
        "    ###################\n",
        "    # train the model #\n",
        "    ###################\n",
        "    model.train() # prep model for training\n",
        "    start = time.time()\n",
        "    # for batch, (data, target) in enumerate(train_loader, 1):\n",
        "    for i, data in enumerate(train_loader):\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        # 前向传播\n",
        "        #print('前向传播')\n",
        "        # forward pass: compute predicted outputs by passing inputs to the model\n",
        "        outputs = model(inputs)\n",
        "        # clear the gradients of all optimized variables\n",
        "        optimizer.zero_grad()\n",
        "        # calculate the loss\n",
        "        loss = criterion(outputs, labels)\n",
        "        # backward pass: compute gradient of the loss with respect to model parameters\n",
        "        loss.backward()\n",
        "        # perform a single optimization step (parameter update)\n",
        "        optimizer.step()\n",
        "        # record training loss\n",
        "        train_losses.append(loss.item())\n",
        "\n",
        "    ######################    \n",
        "    # validate the model #\n",
        "    ######################\n",
        "    model.eval() # prep model for evaluation\n",
        "    # for data, target in valid_loader:\n",
        "    for i, data in enumerate(valid_loader):\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        # forward pass: compute predicted outputs by passing inputs to the model\n",
        "        output = model(inputs)\n",
        "        # calculate the loss\n",
        "        loss = criterion(output, labels)\n",
        "        # record validation loss\n",
        "        valid_losses.append(loss.item())\n",
        "\n",
        "    # print training/validation statistics \n",
        "    # calculate average loss over an epoch\n",
        "    train_loss = np.average(train_losses)\n",
        "    valid_loss = np.average(valid_losses)\n",
        "    avg_train_losses.append(train_loss)\n",
        "    avg_valid_losses.append(valid_loss)\n",
        "    \n",
        "    epoch_len = len(str(n_epochs))\n",
        "    \n",
        "    print_msg = (f'[{epoch:>{epoch_len}}/{n_epochs:>{epoch_len}}] ' +\n",
        "                  f'train_loss: {train_loss:.5f} ' +\n",
        "                  f'valid_loss: {valid_loss:.5f}')\n",
        "    \n",
        "    print(print_msg)\n",
        "    print('using time is :{}'.format(time.time()-start))\n",
        "    # clear lists to track next epoch\n",
        "    train_losses = []\n",
        "    valid_losses = []\n",
        "    \n",
        "    # early_stopping needs the validation loss to check if it has decresed, \n",
        "    # and if it has, it will make a checkpoint of the current model\n",
        "    early_stopping(valid_loss, model)\n",
        "    \n",
        "    if early_stopping.early_stop:\n",
        "        print(\"Early stopping\")\n",
        "        break\n",
        "    \n",
        "# load the last checkpoint with the best model\n",
        "model.load_state_dict(torch.load('checkpoint.pt'))\n",
        "\n",
        "\n",
        "\n",
        "# model, train_loss, valid_loss = train_model(model, batch_size, patience, n_epochs) \n",
        "\n",
        "#保存训练模型\n",
        "torch.save(model, '/content/drive/My Drive/Colab Notebooks/datas/datasets_data/cifar_10_resnet101.pt')\n",
        "model = torch.load('/content/drive/My Drive/Colab Notebooks/datas/datasets_data/cifar_10_resnet101.pt')\n",
        "\n",
        "# 测试\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "for data in test_loader:\n",
        "    images, labels = data\n",
        "    images, labels = images.to(device), labels.to(device)\n",
        "    # 前向传播\n",
        "    out = model(images)\n",
        "    _, predicted = torch.max(out.data, 1)\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "\n",
        "#输出识别准确率\n",
        "print('10000测试图像 准确率:{:.4f}%'.format(100 * correct / total)) "
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[  1/200] train_loss: 2.16753 valid_loss: 0.99205\n",
            "using time is :60.21356439590454\n",
            "Validation loss decreased (inf --> 0.992051).  Saving model ...\n",
            "[  2/200] train_loss: 0.86055 valid_loss: 0.77128\n",
            "using time is :61.411083936691284\n",
            "Validation loss decreased (0.992051 --> 0.771279).  Saving model ...\n",
            "[  3/200] train_loss: 0.68185 valid_loss: 0.68638\n",
            "using time is :61.17520880699158\n",
            "Validation loss decreased (0.771279 --> 0.686382).  Saving model ...\n",
            "[  4/200] train_loss: 0.58052 valid_loss: 0.61823\n",
            "using time is :61.16505789756775\n",
            "Validation loss decreased (0.686382 --> 0.618231).  Saving model ...\n",
            "[  5/200] train_loss: 0.49341 valid_loss: 0.57849\n",
            "using time is :61.430683612823486\n",
            "Validation loss decreased (0.618231 --> 0.578485).  Saving model ...\n",
            "[  6/200] train_loss: 0.43776 valid_loss: 0.58896\n",
            "using time is :61.23532199859619\n",
            "EarlyStopping counter: 1 out of 20\n",
            "[  7/200] train_loss: 0.38413 valid_loss: 0.55810\n",
            "using time is :60.75086855888367\n",
            "Validation loss decreased (0.578485 --> 0.558099).  Saving model ...\n",
            "[  8/200] train_loss: 0.34632 valid_loss: 0.55264\n",
            "using time is :61.03424835205078\n",
            "Validation loss decreased (0.558099 --> 0.552638).  Saving model ...\n",
            "[  9/200] train_loss: 0.31071 valid_loss: 0.56705\n",
            "using time is :61.06881666183472\n",
            "EarlyStopping counter: 1 out of 20\n",
            "[ 10/200] train_loss: 0.28119 valid_loss: 0.55021\n",
            "using time is :60.74437952041626\n",
            "Validation loss decreased (0.552638 --> 0.550214).  Saving model ...\n",
            "[ 11/200] train_loss: 0.25164 valid_loss: 0.55084\n",
            "using time is :61.18777060508728\n",
            "EarlyStopping counter: 1 out of 20\n",
            "[ 12/200] train_loss: 0.22563 valid_loss: 0.55519\n",
            "using time is :60.51525521278381\n",
            "EarlyStopping counter: 2 out of 20\n",
            "[ 13/200] train_loss: 0.20578 valid_loss: 0.58228\n",
            "using time is :60.75062274932861\n",
            "EarlyStopping counter: 3 out of 20\n",
            "[ 14/200] train_loss: 0.18779 valid_loss: 0.58853\n",
            "using time is :60.623125314712524\n",
            "EarlyStopping counter: 4 out of 20\n",
            "[ 15/200] train_loss: 0.18127 valid_loss: 0.57395\n",
            "using time is :60.68289017677307\n",
            "EarlyStopping counter: 5 out of 20\n",
            "[ 16/200] train_loss: 0.16108 valid_loss: 0.56797\n",
            "using time is :61.61508822441101\n",
            "EarlyStopping counter: 6 out of 20\n",
            "[ 17/200] train_loss: 0.15201 valid_loss: 0.59289\n",
            "using time is :61.26016354560852\n",
            "EarlyStopping counter: 7 out of 20\n",
            "[ 18/200] train_loss: 0.13979 valid_loss: 0.57631\n",
            "using time is :61.1553840637207\n",
            "EarlyStopping counter: 8 out of 20\n",
            "[ 19/200] train_loss: 0.13437 valid_loss: 0.59376\n",
            "using time is :61.273443937301636\n",
            "EarlyStopping counter: 9 out of 20\n",
            "[ 20/200] train_loss: 0.12262 valid_loss: 0.60074\n",
            "using time is :61.226099491119385\n",
            "EarlyStopping counter: 10 out of 20\n",
            "[ 21/200] train_loss: 0.12067 valid_loss: 0.61800\n",
            "using time is :61.035675287246704\n",
            "EarlyStopping counter: 11 out of 20\n",
            "[ 22/200] train_loss: 0.12278 valid_loss: 0.61876\n",
            "using time is :61.007638692855835\n",
            "EarlyStopping counter: 12 out of 20\n",
            "[ 23/200] train_loss: 0.11035 valid_loss: 0.61226\n",
            "using time is :61.05240726470947\n",
            "EarlyStopping counter: 13 out of 20\n",
            "[ 24/200] train_loss: 0.10505 valid_loss: 0.59650\n",
            "using time is :60.79334497451782\n",
            "EarlyStopping counter: 14 out of 20\n",
            "[ 25/200] train_loss: 0.09528 valid_loss: 0.61827\n",
            "using time is :60.820876121520996\n",
            "EarlyStopping counter: 15 out of 20\n",
            "[ 26/200] train_loss: 0.08683 valid_loss: 0.59665\n",
            "using time is :60.74727463722229\n",
            "EarlyStopping counter: 16 out of 20\n",
            "[ 27/200] train_loss: 0.08522 valid_loss: 0.61769\n",
            "using time is :60.71166729927063\n",
            "EarlyStopping counter: 17 out of 20\n",
            "[ 28/200] train_loss: 0.09326 valid_loss: 0.63180\n",
            "using time is :60.71822786331177\n",
            "EarlyStopping counter: 18 out of 20\n",
            "[ 29/200] train_loss: 0.08103 valid_loss: 0.61884\n",
            "using time is :60.71457600593567\n",
            "EarlyStopping counter: 19 out of 20\n",
            "[ 30/200] train_loss: 0.07859 valid_loss: 0.61996\n",
            "using time is :60.774534463882446\n",
            "EarlyStopping counter: 20 out of 20\n",
            "Early stopping\n",
            "10000测试图像 准确率:81.6300%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jlb9R-R3zazz",
        "outputId": "b986b0c1-8443-48b1-c20a-0850bc36de72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 585
        }
      },
      "source": [
        "# Visualizing the Loss and the Early Stopping Checkpoint\n",
        "# From the plot we can see that the last Early Stopping Checkpoint was saved right before the model started to overfit.\n",
        "# visualize the loss as the network trained\n",
        "import matplotlib.pyplot as plt\n",
        "train_loss = avg_train_losses\n",
        "valid_loss = avg_valid_losses\n",
        "fig = plt.figure(figsize=(10,8))\n",
        "plt.plot(range(1,len(train_loss)+1),train_loss, label='Training Loss')\n",
        "plt.plot(range(1,len(valid_loss)+1),valid_loss,label='Validation Loss')\n",
        "\n",
        "# find position of lowest validation loss\n",
        "minposs = valid_loss.index(min(valid_loss))+1 \n",
        "plt.axvline(minposs, linestyle='--', color='r',label='Early Stopping Checkpoint')\n",
        "\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.ylim(0, 0.5) # consistent scale\n",
        "plt.xlim(0, len(train_loss)+1) # consistent scale\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "fig.savefig('loss_plot.png', bbox_inches='tight')"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9mkFoyImQeN"
      },
      "source": [
        "# DENSENET121的方式对CIFAR10进行分类预测"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_q6ppDO0mlEQ",
        "outputId": "aa1e33da-a5c8-4bb6-acbc-793e354488e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "cb60ba58354046aba656fab1f27bcd6d",
            "94eed83e949540699894a8874bffc5fd",
            "6674dea4a09f431b98113d7b8823603a",
            "b83ab6f6ed364eb7b2188c84afbcaec0",
            "027cfc5f0a7b432fbf4cf26aeeb49cf4",
            "6baaae45f9b044aeb0290e550568507c",
            "402c6e3907e5477d87e3abcae262fdc7",
            "252cc046295047d3babb14ba8125dc36"
          ]
        }
      },
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.transforms import ToPILImage\n",
        "%matplotlib inline\n",
        "import time\n",
        "show = ToPILImage() #可以把Tensor转换成Image,方便可视化\n",
        "transform = transforms.Compose([  #transforms.Compose就是将对图像处理的方法集中起来\n",
        "    transforms.RandomHorizontalFlip(),#水平翻转\n",
        "    transforms.RandomCrop((32, 32), padding=4),#对图片进行随机裁剪，裁剪的大小是32*32的，填充是4\n",
        "#     参数：size- (sequence or int)，若为sequence,则为(h,w)，若为int，则(size,size)\n",
        "#     padding-(sequence or int, optional)，此参数是设置填充多少个pixel。\n",
        "#     当为int时，图像上下左右均填充int个，例如padding=4，则上下左右均填充4个pixel，若为32x32，则会变成40x40。\n",
        "#     当为sequence时，若有2个数，则第一个数表示左右扩充多少，第二个数表示上下的。当有4个数时，则为左，上，右，下。\n",
        "#     fill- (int or tuple) 填充的值是什么（仅当填充模式为constant时有用）。int时，各通道均填充该值，当长度为3的tuple时，表示RGB通道需要填充的值。\n",
        "    transforms.ToTensor(),#转为Tensor\n",
        "    #在做数据归一化之前必须要把PIL Image转成Tensor，而其他resize或crop操作则不需要。\n",
        "     transforms.Normalize((0.5, 0.5 ,0.5), (0.5, 0.5, 0.5)),#归一化\n",
        "#     归一化操作，这里有两个参数一个是均值，一个是方差，由于是RGB型的所以一个参有三个值，这里面的参数的大小是可调的，调节的公式是：\n",
        "#     class torchvision.transforms.Normalize(mean, std)，\n",
        "#     Normalized_image=(image-mean)/std\n",
        "#     channel=（channel-mean）/std(因为transforms.ToTensor()已经把数据处理成[0,1],那么(x-0.5)/0.5\n",
        "#     就是[-1.0, 1.0])这样一来，我们的数据中的每个值就变成了[-1,1]的数了。\n",
        "    ])\n",
        "\n",
        "# 超参数定义\n",
        "EPOCH = 300               # 训练epoch次数\n",
        "BATCH_SIZE = 200         # 批训练的数量\n",
        "LR = 0.0001              # 学习率\n",
        "DOWNLOAD_MNIST = False  # 设置True 可以自动下载数据\n",
        "\n",
        "# CIFAR-10数据集下载\n",
        "train_data = datasets.CIFAR10(root='/content/drive/My Drive/Colab Notebooks/datas/datasets_data',\n",
        "                         train=True,                         # 训练集\n",
        "                         # 数据变换(0, 255) -> (0, 1)\n",
        "                         transform=transform   \n",
        "                           )\n",
        "\n",
        "test_data = datasets.CIFAR10(root='/content/drive/My Drive/Colab Notebooks/datas/datasets_data',\n",
        "                        train=False,                         # 测试集\n",
        "                        transform=transform\n",
        "                        #download=True\n",
        "                          )\n",
        "\n",
        "# 使用DataLoader进行分批\n",
        "train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_data, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "#数据集10个类的定义\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "# densenet121 Model\n",
        "model = torchvision.models.densenet121(pretrained=True)\n",
        "#损失函数:这里用交叉熵\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "#优化器 Adam\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "#device : GPU or CPU\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "print('Start Training')\n",
        "# 训练\n",
        "\n",
        "for epoch in range(EPOCH):\n",
        "    running_loss = 0.0\n",
        "    start = time.time()\n",
        "    for i, data in enumerate(train_loader):#enumerate枚举数据并从下标0开始\n",
        "        # 读取数据的数据内容和标签\n",
        "        #print('读取数据的数据内容和标签')\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        # 前向传播\n",
        "        #print('前向传播')\n",
        "        outputs = model(inputs)\n",
        "        # 计算损失函数\n",
        "        #print('计算损失函数')\n",
        "        loss = criterion(outputs, labels)\n",
        "        #梯度清零，也就是把loss关于weight的导数变成0.\n",
        "        optimizer.zero_grad()\n",
        "        # 反向传播\n",
        "        loss.backward()\n",
        "        # 参数更新\n",
        "        optimizer.step()\n",
        "        # #打印log信息\n",
        "        # running_loss += loss.item()# #用于从tensor中获取python数字\n",
        "        # if i % 2000 == 1999:#每2000个batch打印一次训练状态\n",
        "        #     print('[%d, %5d] loss: %.3f' \\\n",
        "        #          % (epoch+1, i+1, running_loss / 2000))\n",
        "\n",
        "        #     running_loss = 0.0\n",
        "#print('Finished Training')\n",
        "    end = time.time()\n",
        "    print('epoch{} loss:{:.4f} using time is {:.2f}s'.format(epoch+1, loss.item(), (end-start)))\n",
        "    if loss.item() < 0.001:\n",
        "      print('Training Stop')\n",
        "      break\n",
        "print(\"Finished Traning\")\n",
        "\n",
        "\n",
        "#保存训练模型\n",
        "torch.save(model, '/content/drive/My Drive/Colab Notebooks/datas/datasets_data/cifar_10_dense121.pt')\n",
        "model = torch.load('/content/drive/My Drive/Colab Notebooks/datas/datasets_data/cifar_10_dense121.pt')\n",
        "\n",
        "# 测试\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "for data in test_loader:\n",
        "    images, labels = data\n",
        "    images, labels = images.to(device), labels.to(device)\n",
        "    # 前向传播\n",
        "    out = model(images)\n",
        "    _, predicted = torch.max(out.data, 1)\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "\n",
        "#输出识别准确率\n",
        "print('10000测试图像 准确率:{:.4f}%'.format(100 * correct / total)) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/densenet121-a639ec97.pth\" to /root/.cache/torch/hub/checkpoints/densenet121-a639ec97.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cb60ba58354046aba656fab1f27bcd6d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=32342954.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Start Training\n",
            "epoch1 loss:0.9866 using time is 47.39s\n",
            "epoch2 loss:0.8631 using time is 47.12s\n",
            "epoch3 loss:0.5307 using time is 47.25s\n",
            "epoch4 loss:0.3829 using time is 47.17s\n",
            "epoch5 loss:0.5108 using time is 47.25s\n",
            "epoch6 loss:0.3835 using time is 47.23s\n",
            "epoch7 loss:0.3623 using time is 47.17s\n",
            "epoch8 loss:0.2443 using time is 47.13s\n",
            "epoch9 loss:0.3477 using time is 47.11s\n",
            "epoch10 loss:0.2746 using time is 47.25s\n",
            "epoch11 loss:0.2666 using time is 47.25s\n",
            "epoch12 loss:0.2596 using time is 47.31s\n",
            "epoch13 loss:0.3144 using time is 47.55s\n",
            "epoch14 loss:0.1757 using time is 47.44s\n",
            "epoch15 loss:0.1547 using time is 47.34s\n",
            "epoch16 loss:0.1369 using time is 47.53s\n",
            "epoch17 loss:0.1344 using time is 47.39s\n",
            "epoch18 loss:0.1838 using time is 47.03s\n",
            "epoch19 loss:0.1789 using time is 47.16s\n",
            "epoch20 loss:0.1635 using time is 47.09s\n",
            "epoch21 loss:0.1228 using time is 47.00s\n",
            "epoch22 loss:0.0778 using time is 46.99s\n",
            "epoch23 loss:0.0753 using time is 47.05s\n",
            "epoch24 loss:0.1140 using time is 47.00s\n",
            "epoch25 loss:0.0430 using time is 46.82s\n",
            "epoch26 loss:0.1333 using time is 46.96s\n",
            "epoch27 loss:0.0758 using time is 46.83s\n",
            "epoch28 loss:0.0853 using time is 46.92s\n",
            "epoch29 loss:0.0956 using time is 46.86s\n",
            "epoch30 loss:0.0330 using time is 46.86s\n",
            "epoch31 loss:0.0899 using time is 46.99s\n",
            "epoch32 loss:0.0544 using time is 47.00s\n",
            "epoch33 loss:0.0585 using time is 46.89s\n",
            "epoch34 loss:0.0442 using time is 46.91s\n",
            "epoch35 loss:0.0585 using time is 46.97s\n",
            "epoch36 loss:0.0389 using time is 46.94s\n",
            "epoch37 loss:0.0738 using time is 47.08s\n",
            "epoch38 loss:0.0494 using time is 47.16s\n",
            "epoch39 loss:0.0466 using time is 47.15s\n",
            "epoch40 loss:0.0392 using time is 47.12s\n",
            "epoch41 loss:0.0242 using time is 47.14s\n",
            "epoch42 loss:0.0697 using time is 47.08s\n",
            "epoch43 loss:0.0258 using time is 47.11s\n",
            "epoch44 loss:0.0162 using time is 47.12s\n",
            "epoch45 loss:0.0508 using time is 46.89s\n",
            "epoch46 loss:0.0532 using time is 46.92s\n",
            "epoch47 loss:0.0395 using time is 46.92s\n",
            "epoch48 loss:0.0607 using time is 46.74s\n",
            "epoch49 loss:0.0524 using time is 47.13s\n",
            "epoch50 loss:0.0324 using time is 47.11s\n",
            "epoch51 loss:0.0106 using time is 46.94s\n",
            "epoch52 loss:0.0594 using time is 47.12s\n",
            "epoch53 loss:0.0936 using time is 47.05s\n",
            "epoch54 loss:0.0752 using time is 46.96s\n",
            "epoch55 loss:0.0352 using time is 47.10s\n",
            "epoch56 loss:0.0179 using time is 47.10s\n",
            "epoch57 loss:0.0824 using time is 47.15s\n",
            "epoch58 loss:0.1306 using time is 47.21s\n",
            "epoch59 loss:0.0330 using time is 47.44s\n",
            "epoch60 loss:0.0360 using time is 47.18s\n",
            "epoch61 loss:0.0202 using time is 47.25s\n",
            "epoch62 loss:0.0163 using time is 47.26s\n",
            "epoch63 loss:0.0238 using time is 47.12s\n",
            "epoch64 loss:0.0173 using time is 47.12s\n",
            "epoch65 loss:0.0131 using time is 47.14s\n",
            "epoch66 loss:0.0354 using time is 47.28s\n",
            "epoch67 loss:0.0151 using time is 47.24s\n",
            "epoch68 loss:0.0149 using time is 47.23s\n",
            "epoch69 loss:0.0762 using time is 47.33s\n",
            "epoch70 loss:0.0150 using time is 47.18s\n",
            "epoch71 loss:0.0385 using time is 47.13s\n",
            "epoch72 loss:0.0421 using time is 47.24s\n",
            "epoch73 loss:0.0128 using time is 47.28s\n",
            "epoch74 loss:0.0264 using time is 49.25s\n",
            "epoch75 loss:0.0219 using time is 48.03s\n",
            "epoch76 loss:0.0328 using time is 48.34s\n",
            "epoch77 loss:0.0539 using time is 48.70s\n",
            "epoch78 loss:0.0247 using time is 48.68s\n",
            "epoch79 loss:0.0246 using time is 48.64s\n",
            "epoch80 loss:0.0139 using time is 48.77s\n",
            "epoch81 loss:0.0059 using time is 47.35s\n",
            "epoch82 loss:0.0153 using time is 46.24s\n",
            "epoch83 loss:0.0086 using time is 46.13s\n",
            "epoch84 loss:0.0484 using time is 46.59s\n",
            "epoch85 loss:0.0254 using time is 47.49s\n",
            "epoch86 loss:0.0095 using time is 47.83s\n",
            "epoch87 loss:0.0147 using time is 47.69s\n",
            "epoch88 loss:0.0203 using time is 46.64s\n",
            "epoch89 loss:0.0309 using time is 46.57s\n",
            "epoch90 loss:0.0359 using time is 46.50s\n",
            "epoch91 loss:0.0067 using time is 46.60s\n",
            "epoch92 loss:0.0080 using time is 47.31s\n",
            "epoch93 loss:0.0089 using time is 48.12s\n",
            "epoch94 loss:0.0684 using time is 48.35s\n",
            "epoch95 loss:0.0038 using time is 48.11s\n",
            "epoch96 loss:0.0049 using time is 47.99s\n",
            "epoch97 loss:0.0043 using time is 48.30s\n",
            "epoch98 loss:0.0055 using time is 48.38s\n",
            "epoch99 loss:0.0328 using time is 48.29s\n",
            "epoch100 loss:0.0328 using time is 48.37s\n",
            "epoch101 loss:0.0046 using time is 48.44s\n",
            "epoch102 loss:0.0248 using time is 48.12s\n",
            "epoch103 loss:0.0056 using time is 48.06s\n",
            "epoch104 loss:0.0294 using time is 48.08s\n",
            "epoch105 loss:0.0247 using time is 48.30s\n",
            "epoch106 loss:0.0216 using time is 48.12s\n",
            "epoch107 loss:0.0060 using time is 48.26s\n",
            "epoch108 loss:0.0263 using time is 48.29s\n",
            "epoch109 loss:0.0134 using time is 48.26s\n",
            "epoch110 loss:0.0049 using time is 48.18s\n",
            "epoch111 loss:0.0099 using time is 48.28s\n",
            "epoch112 loss:0.0120 using time is 47.93s\n",
            "epoch113 loss:0.0164 using time is 48.01s\n",
            "epoch114 loss:0.0537 using time is 48.10s\n",
            "epoch115 loss:0.0336 using time is 48.04s\n",
            "epoch116 loss:0.0045 using time is 48.01s\n",
            "epoch117 loss:0.0195 using time is 48.22s\n",
            "epoch118 loss:0.0291 using time is 48.54s\n",
            "epoch119 loss:0.0077 using time is 48.40s\n",
            "epoch120 loss:0.0339 using time is 47.74s\n",
            "epoch121 loss:0.0183 using time is 47.77s\n",
            "epoch122 loss:0.0022 using time is 47.76s\n",
            "epoch123 loss:0.0242 using time is 47.90s\n",
            "epoch124 loss:0.0205 using time is 47.80s\n",
            "epoch125 loss:0.0147 using time is 47.61s\n",
            "epoch126 loss:0.0166 using time is 47.82s\n",
            "epoch127 loss:0.0125 using time is 47.50s\n",
            "epoch128 loss:0.0222 using time is 47.26s\n",
            "epoch129 loss:0.0363 using time is 47.18s\n",
            "epoch130 loss:0.0066 using time is 47.38s\n",
            "epoch131 loss:0.0034 using time is 47.27s\n",
            "epoch132 loss:0.0054 using time is 47.57s\n",
            "epoch133 loss:0.0069 using time is 47.32s\n",
            "epoch134 loss:0.0051 using time is 47.37s\n",
            "epoch135 loss:0.0212 using time is 47.39s\n",
            "epoch136 loss:0.0045 using time is 47.28s\n",
            "epoch137 loss:0.0133 using time is 47.57s\n",
            "epoch138 loss:0.0085 using time is 47.53s\n",
            "epoch139 loss:0.0160 using time is 47.35s\n",
            "epoch140 loss:0.0237 using time is 47.39s\n",
            "epoch141 loss:0.0032 using time is 47.56s\n",
            "epoch142 loss:0.0051 using time is 47.19s\n",
            "epoch143 loss:0.0215 using time is 47.16s\n",
            "epoch144 loss:0.0011 using time is 47.18s\n",
            "epoch145 loss:0.0017 using time is 47.18s\n",
            "epoch146 loss:0.0119 using time is 46.93s\n",
            "epoch147 loss:0.0079 using time is 47.00s\n",
            "epoch148 loss:0.0130 using time is 46.83s\n",
            "epoch149 loss:0.0144 using time is 47.21s\n",
            "epoch150 loss:0.0087 using time is 47.14s\n",
            "epoch151 loss:0.0218 using time is 46.97s\n",
            "epoch152 loss:0.0065 using time is 46.95s\n",
            "epoch153 loss:0.0068 using time is 46.99s\n",
            "epoch154 loss:0.0033 using time is 46.87s\n",
            "epoch155 loss:0.0327 using time is 46.96s\n",
            "epoch156 loss:0.0509 using time is 46.81s\n",
            "epoch157 loss:0.0184 using time is 46.87s\n",
            "epoch158 loss:0.0076 using time is 46.92s\n",
            "epoch159 loss:0.0065 using time is 46.69s\n",
            "epoch160 loss:0.0020 using time is 46.97s\n",
            "epoch161 loss:0.0138 using time is 46.91s\n",
            "epoch162 loss:0.0068 using time is 46.95s\n",
            "epoch163 loss:0.0343 using time is 46.89s\n",
            "epoch164 loss:0.0024 using time is 46.94s\n",
            "epoch165 loss:0.0111 using time is 46.99s\n",
            "epoch166 loss:0.0041 using time is 46.88s\n",
            "epoch167 loss:0.0161 using time is 46.86s\n",
            "epoch168 loss:0.0054 using time is 46.83s\n",
            "epoch169 loss:0.0260 using time is 46.84s\n",
            "epoch170 loss:0.0025 using time is 47.05s\n",
            "epoch171 loss:0.0073 using time is 47.09s\n",
            "epoch172 loss:0.0046 using time is 47.01s\n",
            "epoch173 loss:0.0037 using time is 47.00s\n",
            "epoch174 loss:0.0047 using time is 47.10s\n",
            "epoch175 loss:0.0147 using time is 46.96s\n",
            "epoch176 loss:0.0361 using time is 47.09s\n",
            "epoch177 loss:0.0353 using time is 48.30s\n",
            "epoch178 loss:0.0006 using time is 48.50s\n",
            "Training Stop\n",
            "Finished Traning\n",
            "10000测试图像 准确率:87.4200%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75Qf6WdEpVhU"
      },
      "source": [
        "# DENSENET201的方式对CIFAR10进行分类预测(Early Stopping)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g051gEi0pa7S",
        "outputId": "b0579b6c-bd99-49ff-c3f5-389996e56315",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "4895e98e91364499901a5bcb0c0b13a3",
            "9b49249894cd45c0a85cc1438c8e14e8",
            "62c150fda84e422bbeb4af1eb47d5569",
            "a624cd6f577c4d4fb3e635fd14b9dee4",
            "7ca4e13fb13342a8b83f7e67b08eabbe",
            "c376c99546a94f79a76c2bc74275a05a",
            "d42e84542463476fb229e9534adf8d15",
            "a9c5809c0b844e8397e66bf77911fc87"
          ]
        }
      },
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.transforms import ToPILImage\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import pytorchtools\n",
        "%matplotlib inline\n",
        "import time\n",
        "import numpy as np\n",
        "show = ToPILImage() #可以把Tensor转换成Image,方便可视化\n",
        "transform = transforms.Compose([  #transforms.Compose就是将对图像处理的方法集中起来\n",
        "    transforms.RandomHorizontalFlip(),#水平翻转\n",
        "    transforms.RandomCrop((32, 32), padding=4),#对图片进行随机裁剪，裁剪的大小是32*32的，填充是4\n",
        "#     参数：size- (sequence or int)，若为sequence,则为(h,w)，若为int，则(size,size)\n",
        "#     padding-(sequence or int, optional)，此参数是设置填充多少个pixel。\n",
        "#     当为int时，图像上下左右均填充int个，例如padding=4，则上下左右均填充4个pixel，若为32x32，则会变成40x40。\n",
        "#     当为sequence时，若有2个数，则第一个数表示左右扩充多少，第二个数表示上下的。当有4个数时，则为左，上，右，下。\n",
        "#     fill- (int or tuple) 填充的值是什么（仅当填充模式为constant时有用）。int时，各通道均填充该值，当长度为3的tuple时，表示RGB通道需要填充的值。\n",
        "    transforms.ToTensor(),#转为Tensor\n",
        "    #在做数据归一化之前必须要把PIL Image转成Tensor，而其他resize或crop操作则不需要。\n",
        "     transforms.Normalize((0.5, 0.5 ,0.5), (0.5, 0.5, 0.5)),#归一化\n",
        "#     归一化操作，这里有两个参数一个是均值，一个是方差，由于是RGB型的所以一个参有三个值，这里面的参数的大小是可调的，调节的公式是：\n",
        "#     class torchvision.transforms.Normalize(mean, std)，\n",
        "#     Normalized_image=(image-mean)/std\n",
        "#     channel=（channel-mean）/std(因为transforms.ToTensor()已经把数据处理成[0,1],那么(x-0.5)/0.5\n",
        "#     就是[-1.0, 1.0])这样一来，我们的数据中的每个值就变成了[-1,1]的数了。\n",
        "    ])\n",
        "\n",
        "# CIFAR-10数据集下载\n",
        "train_data = datasets.CIFAR10(root='/content/drive/My Drive/Colab Notebooks/datas/datasets_data',\n",
        "                         train=True,                         # 训练集\n",
        "                         # 数据变换(0, 255) -> (0, 1)\n",
        "                         transform=transform   \n",
        "                           )\n",
        "\n",
        "test_data = datasets.CIFAR10(root='/content/drive/My Drive/Colab Notebooks/datas/datasets_data',\n",
        "                        train=False,                         # 测试集\n",
        "                        transform=transform\n",
        "                        #download=True\n",
        "                          )\n",
        "\n",
        "#划分训练集用于生成验证集\n",
        "valid_size = 0.2\n",
        "num_train = len(train_data)\n",
        "indices = list(range(num_train))\n",
        "np.random.shuffle(indices)\n",
        "split = int(np.floor(valid_size * num_train))\n",
        "train_idx, valid_idx = indices[split:], indices[:split]\n",
        "\n",
        "#定义用于获取训练和验证批次的采样器\n",
        "train_sampler = SubsetRandomSampler(train_idx)\n",
        "valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "# 使用DataLoader进行分批\n",
        "train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, sampler=train_sampler, num_workers=0)\n",
        "valid_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, sampler=valid_sampler, num_workers=0)\n",
        "test_loader = DataLoader(dataset=test_data, batch_size=BATCH_SIZE)\n",
        "\n",
        "#数据集10个类的定义\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "\n",
        "# from pytorchtools import EarlyStopping\n",
        "class EarlyStopping:\n",
        "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
        "    def __init__(self, patience=7, verbose=False, delta=0):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How long to wait after last time validation loss improved.\n",
        "                            Default: 7\n",
        "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
        "                            Default: False\n",
        "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
        "                            Default: 0\n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.delta = delta\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''Saves model when validation loss decrease.'''\n",
        "        if self.verbose:\n",
        "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        torch.save(model.state_dict(), 'checkpoint.pt')\t# 这里会存储迄今最优模型的参数\n",
        "        self.val_loss_min = val_loss\n",
        "\n",
        "# densenet201 Model\n",
        "model = torchvision.models.densenet201(pretrained=True)\n",
        "\n",
        "batch_size = 256\n",
        "n_epochs = 200\n",
        "\n",
        "# early stopping patience; how long to wait after last time validation loss improved.\n",
        "patience = 20\n",
        "    \n",
        "#损失函数:这里用交叉熵\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "#优化器 Adam\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "#device : GPU or CPU\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# to track the training loss as the model trains\n",
        "train_losses = []\n",
        "# to track the validation loss as the model trains\n",
        "valid_losses = []\n",
        "# to track the average training loss per epoch as the model trains\n",
        "avg_train_losses = []\n",
        "# to track the average validation loss per epoch as the model trains\n",
        "avg_valid_losses = [] \n",
        "\n",
        "# initialize the early_stopping object\n",
        "early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
        "\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "\n",
        "    ###################\n",
        "    # train the model #\n",
        "    ###################\n",
        "    model.train() # prep model for training\n",
        "    start = time.time()\n",
        "    # for batch, (data, target) in enumerate(train_loader, 1):\n",
        "    for i, data in enumerate(train_loader):\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        # 前向传播\n",
        "        #print('前向传播')\n",
        "        # forward pass: compute predicted outputs by passing inputs to the model\n",
        "        outputs = model(inputs)\n",
        "        # clear the gradients of all optimized variables\n",
        "        optimizer.zero_grad()\n",
        "        # calculate the loss\n",
        "        loss = criterion(outputs, labels)\n",
        "        # backward pass: compute gradient of the loss with respect to model parameters\n",
        "        loss.backward()\n",
        "        # perform a single optimization step (parameter update)\n",
        "        optimizer.step()\n",
        "        # record training loss\n",
        "        train_losses.append(loss.item())\n",
        "\n",
        "    ######################    \n",
        "    # validate the model #\n",
        "    ######################\n",
        "    model.eval() # prep model for evaluation\n",
        "    # for data, target in valid_loader:\n",
        "    for i, data in enumerate(valid_loader):\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        # forward pass: compute predicted outputs by passing inputs to the model\n",
        "        output = model(inputs)\n",
        "        # calculate the loss\n",
        "        loss = criterion(output, labels)\n",
        "        # record validation loss\n",
        "        valid_losses.append(loss.item())\n",
        "\n",
        "    # print training/validation statistics \n",
        "    # calculate average loss over an epoch\n",
        "    train_loss = np.average(train_losses)\n",
        "    valid_loss = np.average(valid_losses)\n",
        "    avg_train_losses.append(train_loss)\n",
        "    avg_valid_losses.append(valid_loss)\n",
        "    \n",
        "    epoch_len = len(str(n_epochs))\n",
        "    \n",
        "    print_msg = (f'[{epoch:>{epoch_len}}/{n_epochs:>{epoch_len}}] ' +\n",
        "                  f'train_loss: {train_loss:.5f} ' +\n",
        "                  f'valid_loss: {valid_loss:.5f}')\n",
        "    \n",
        "    print(print_msg)\n",
        "    print('using time is :{}'.format(time.time()-start))\n",
        "    # clear lists to track next epoch\n",
        "    train_losses = []\n",
        "    valid_losses = []\n",
        "    \n",
        "    # early_stopping needs the validation loss to check if it has decresed, \n",
        "    # and if it has, it will make a checkpoint of the current model\n",
        "    early_stopping(valid_loss, model)\n",
        "    \n",
        "    if early_stopping.early_stop:\n",
        "        print(\"Early stopping\")\n",
        "        break\n",
        "    \n",
        "# load the last checkpoint with the best model\n",
        "model.load_state_dict(torch.load('checkpoint.pt'))\n",
        "\n",
        "\n",
        "\n",
        "# model, train_loss, valid_loss = train_model(model, batch_size, patience, n_epochs) \n",
        "\n",
        "#保存训练模型\n",
        "torch.save(model, '/content/drive/My Drive/Colab Notebooks/datas/datasets_data/cifar_10_resnet101.pt')\n",
        "model = torch.load('/content/drive/My Drive/Colab Notebooks/datas/datasets_data/cifar_10_resnet101.pt')\n",
        "\n",
        "# 测试\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "for data in test_loader:\n",
        "    images, labels = data\n",
        "    images, labels = images.to(device), labels.to(device)\n",
        "    # 前向传播\n",
        "    out = model(images)\n",
        "    _, predicted = torch.max(out.data, 1)\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "\n",
        "#输出识别准确率\n",
        "print('10000测试图像 准确率:{:.4f}%'.format(100 * correct / total)) "
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/densenet201-c1103571.pth\" to /root/.cache/torch/hub/checkpoints/densenet201-c1103571.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4895e98e91364499901a5bcb0c0b13a3",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=81131730.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[  1/200] train_loss: 2.38359 valid_loss: 0.93257\n",
            "using time is :59.490923166275024\n",
            "Validation loss decreased (inf --> 0.932568).  Saving model ...\n",
            "[  2/200] train_loss: 0.80062 valid_loss: 0.74390\n",
            "using time is :59.6456081867218\n",
            "Validation loss decreased (0.932568 --> 0.743898).  Saving model ...\n",
            "[  3/200] train_loss: 0.61809 valid_loss: 0.63135\n",
            "using time is :59.81680417060852\n",
            "Validation loss decreased (0.743898 --> 0.631350).  Saving model ...\n",
            "[  4/200] train_loss: 0.51543 valid_loss: 0.56782\n",
            "using time is :59.77269244194031\n",
            "Validation loss decreased (0.631350 --> 0.567822).  Saving model ...\n",
            "[  5/200] train_loss: 0.42959 valid_loss: 0.55006\n",
            "using time is :59.615387201309204\n",
            "Validation loss decreased (0.567822 --> 0.550058).  Saving model ...\n",
            "[  6/200] train_loss: 0.36364 valid_loss: 0.55732\n",
            "using time is :59.785096645355225\n",
            "EarlyStopping counter: 1 out of 20\n",
            "[  7/200] train_loss: 0.30178 valid_loss: 0.52587\n",
            "using time is :59.52797842025757\n",
            "Validation loss decreased (0.550058 --> 0.525866).  Saving model ...\n",
            "[  8/200] train_loss: 0.25592 valid_loss: 0.52576\n",
            "using time is :59.44201612472534\n",
            "Validation loss decreased (0.525866 --> 0.525758).  Saving model ...\n",
            "[  9/200] train_loss: 0.21441 valid_loss: 0.53921\n",
            "using time is :59.511592864990234\n",
            "EarlyStopping counter: 1 out of 20\n",
            "[ 10/200] train_loss: 0.18032 valid_loss: 0.54393\n",
            "using time is :59.390825271606445\n",
            "EarlyStopping counter: 2 out of 20\n",
            "[ 11/200] train_loss: 0.15076 valid_loss: 0.56701\n",
            "using time is :59.10155630111694\n",
            "EarlyStopping counter: 3 out of 20\n",
            "[ 12/200] train_loss: 0.13685 valid_loss: 0.55231\n",
            "using time is :59.208338499069214\n",
            "EarlyStopping counter: 4 out of 20\n",
            "[ 13/200] train_loss: 0.11674 valid_loss: 0.58971\n",
            "using time is :59.295164346694946\n",
            "EarlyStopping counter: 5 out of 20\n",
            "[ 14/200] train_loss: 0.10284 valid_loss: 0.57441\n",
            "using time is :59.20367789268494\n",
            "EarlyStopping counter: 6 out of 20\n",
            "[ 15/200] train_loss: 0.09483 valid_loss: 0.57453\n",
            "using time is :59.249598264694214\n",
            "EarlyStopping counter: 7 out of 20\n",
            "[ 16/200] train_loss: 0.08215 valid_loss: 0.57499\n",
            "using time is :58.85895490646362\n",
            "EarlyStopping counter: 8 out of 20\n",
            "[ 17/200] train_loss: 0.07847 valid_loss: 0.56581\n",
            "using time is :58.94132709503174\n",
            "EarlyStopping counter: 9 out of 20\n",
            "[ 18/200] train_loss: 0.06953 valid_loss: 0.58694\n",
            "using time is :58.86470818519592\n",
            "EarlyStopping counter: 10 out of 20\n",
            "[ 19/200] train_loss: 0.06177 valid_loss: 0.62971\n",
            "using time is :59.04109334945679\n",
            "EarlyStopping counter: 11 out of 20\n",
            "[ 20/200] train_loss: 0.05985 valid_loss: 0.61874\n",
            "using time is :59.0154013633728\n",
            "EarlyStopping counter: 12 out of 20\n",
            "[ 21/200] train_loss: 0.05883 valid_loss: 0.60565\n",
            "using time is :58.81286811828613\n",
            "EarlyStopping counter: 13 out of 20\n",
            "[ 22/200] train_loss: 0.05461 valid_loss: 0.65417\n",
            "using time is :58.86311435699463\n",
            "EarlyStopping counter: 14 out of 20\n",
            "[ 23/200] train_loss: 0.05321 valid_loss: 0.61044\n",
            "using time is :58.929797410964966\n",
            "EarlyStopping counter: 15 out of 20\n",
            "[ 24/200] train_loss: 0.05102 valid_loss: 0.63863\n",
            "using time is :58.72679162025452\n",
            "EarlyStopping counter: 16 out of 20\n",
            "[ 25/200] train_loss: 0.05012 valid_loss: 0.63479\n",
            "using time is :58.91842436790466\n",
            "EarlyStopping counter: 17 out of 20\n",
            "[ 26/200] train_loss: 0.04726 valid_loss: 0.61819\n",
            "using time is :58.64718794822693\n",
            "EarlyStopping counter: 18 out of 20\n",
            "[ 28/200] train_loss: 0.04547 valid_loss: 0.65433\n",
            "using time is :58.61824107170105\n",
            "EarlyStopping counter: 20 out of 20\n",
            "Early stopping\n",
            "10000测试图像 准确率:82.6100%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8N4m7m50Xpr"
      },
      "source": [
        "# DenseNet201 lr 0.0001"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTtUWvSl0SaR",
        "outputId": "e1211e4e-b702-4884-8723-8cb8ad61a65e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        }
      },
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.transforms import ToPILImage\n",
        "%matplotlib inline\n",
        "import time\n",
        "show = ToPILImage() #可以把Tensor转换成Image,方便可视化\n",
        "transform = transforms.Compose([  #transforms.Compose就是将对图像处理的方法集中起来\n",
        "    transforms.RandomHorizontalFlip(),#水平翻转\n",
        "    transforms.RandomCrop((32, 32), padding=4),#对图片进行随机裁剪，裁剪的大小是32*32的，填充是4\n",
        "#     参数：size- (sequence or int)，若为sequence,则为(h,w)，若为int，则(size,size)\n",
        "#     padding-(sequence or int, optional)，此参数是设置填充多少个pixel。\n",
        "#     当为int时，图像上下左右均填充int个，例如padding=4，则上下左右均填充4个pixel，若为32x32，则会变成40x40。\n",
        "#     当为sequence时，若有2个数，则第一个数表示左右扩充多少，第二个数表示上下的。当有4个数时，则为左，上，右，下。\n",
        "#     fill- (int or tuple) 填充的值是什么（仅当填充模式为constant时有用）。int时，各通道均填充该值，当长度为3的tuple时，表示RGB通道需要填充的值。\n",
        "    transforms.ToTensor(),#转为Tensor\n",
        "    #在做数据归一化之前必须要把PIL Image转成Tensor，而其他resize或crop操作则不需要。\n",
        "     transforms.Normalize((0.5, 0.5 ,0.5), (0.5, 0.5, 0.5)),#归一化\n",
        "#     归一化操作，这里有两个参数一个是均值，一个是方差，由于是RGB型的所以一个参有三个值，这里面的参数的大小是可调的，调节的公式是：\n",
        "#     class torchvision.transforms.Normalize(mean, std)，\n",
        "#     Normalized_image=(image-mean)/std\n",
        "#     channel=（channel-mean）/std(因为transforms.ToTensor()已经把数据处理成[0,1],那么(x-0.5)/0.5\n",
        "#     就是[-1.0, 1.0])这样一来，我们的数据中的每个值就变成了[-1,1]的数了。\n",
        "    ])\n",
        "\n",
        "# 超参数定义\n",
        "EPOCH = 100               # 训练epoch次数\n",
        "BATCH_SIZE = 200         # 批训练的数量\n",
        "LR = 0.0001              # 学习率\n",
        "DOWNLOAD_MNIST = False  # 设置True 可以自动下载数据\n",
        "\n",
        "# CIFAR-10数据集下载\n",
        "train_data = datasets.CIFAR10(root='/content/drive/My Drive/Colab Notebooks/datas/datasets_data',\n",
        "                         train=True,                         # 训练集\n",
        "                         # 数据变换(0, 255) -> (0, 1)\n",
        "                         transform=transform   \n",
        "                           )\n",
        "\n",
        "test_data = datasets.CIFAR10(root='/content/drive/My Drive/Colab Notebooks/datas/datasets_data',\n",
        "                        train=False,                         # 测试集\n",
        "                        transform=transform\n",
        "                        #download=True\n",
        "                          )\n",
        "\n",
        "# 使用DataLoader进行分批\n",
        "train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_data, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "#数据集10个类的定义\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "# densenet201 Model\n",
        "model = torchvision.models.densenet201(pretrained=True)\n",
        "#损失函数:这里用交叉熵\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "#优化器 Adam\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "#device : GPU or CPU\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "print('Start Training')\n",
        "# 训练\n",
        "\n",
        "for epoch in range(EPOCH):\n",
        "    running_loss = 0.0\n",
        "    start = time.time()\n",
        "    for i, data in enumerate(train_loader):#enumerate枚举数据并从下标0开始\n",
        "        # 读取数据的数据内容和标签\n",
        "        #print('读取数据的数据内容和标签')\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        # 前向传播\n",
        "        #print('前向传播')\n",
        "        outputs = model(inputs)\n",
        "        # 计算损失函数\n",
        "        #print('计算损失函数')\n",
        "        loss = criterion(outputs, labels)\n",
        "        #梯度清零，也就是把loss关于weight的导数变成0.\n",
        "        optimizer.zero_grad()\n",
        "        # 反向传播\n",
        "        loss.backward()\n",
        "        # 参数更新\n",
        "        optimizer.step()\n",
        "        # #打印log信息\n",
        "        # running_loss += loss.item()# #用于从tensor中获取python数字\n",
        "        # if i % 2000 == 1999:#每2000个batch打印一次训练状态\n",
        "        #     print('[%d, %5d] loss: %.3f' \\\n",
        "        #          % (epoch+1, i+1, running_loss / 2000))\n",
        "\n",
        "        #     running_loss = 0.0\n",
        "#print('Finished Training')\n",
        "    end = time.time()\n",
        "    print('epoch{} loss:{:.4f} using time is {:.2f}s'.format(epoch+1, loss.item(), (end-start)))\n",
        "    if loss.item() < 0.05:\n",
        "      print('Training Stop')\n",
        "      break\n",
        "print(\"Finished Traning\")\n",
        "\n",
        "\n",
        "#保存训练模型\n",
        "torch.save(model, '/content/drive/My Drive/Colab Notebooks/datas/datasets_data/cifar_10_dense201.pt')\n",
        "model = torch.load('/content/drive/My Drive/Colab Notebooks/datas/datasets_data/cifar_10_dense201.pt')\n",
        "\n",
        "# 测试\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "for data in test_loader:\n",
        "    images, labels = data\n",
        "    images, labels = images.to(device), labels.to(device)\n",
        "    # 前向传播\n",
        "    out = model(images)\n",
        "    _, predicted = torch.max(out.data, 1)\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "\n",
        "#输出识别准确率\n",
        "print('10000测试图像 准确率:{:.4f}%'.format(100 * correct / total)) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start Training\n",
            "epoch1 loss:0.9641 using time is 142.08s\n",
            "epoch2 loss:0.6992 using time is 142.10s\n",
            "epoch3 loss:0.5758 using time is 142.06s\n",
            "epoch4 loss:0.4800 using time is 142.11s\n",
            "epoch5 loss:0.4798 using time is 140.64s\n",
            "epoch6 loss:0.2209 using time is 140.59s\n",
            "epoch7 loss:0.3321 using time is 140.28s\n",
            "epoch8 loss:0.2763 using time is 141.65s\n",
            "epoch9 loss:0.1889 using time is 142.06s\n",
            "epoch10 loss:0.1396 using time is 142.42s\n",
            "epoch11 loss:0.1139 using time is 142.27s\n",
            "epoch12 loss:0.1575 using time is 142.29s\n",
            "epoch13 loss:0.2051 using time is 141.63s\n",
            "epoch14 loss:0.0781 using time is 141.93s\n",
            "epoch15 loss:0.0959 using time is 142.19s\n",
            "epoch16 loss:0.0855 using time is 141.87s\n",
            "epoch17 loss:0.0961 using time is 142.04s\n",
            "epoch18 loss:0.1600 using time is 142.10s\n",
            "epoch19 loss:0.0747 using time is 141.78s\n",
            "epoch20 loss:0.1118 using time is 141.96s\n",
            "epoch21 loss:0.0672 using time is 142.21s\n",
            "epoch22 loss:0.0543 using time is 142.57s\n",
            "epoch23 loss:0.0690 using time is 142.37s\n",
            "epoch24 loss:0.0354 using time is 142.51s\n",
            "Training Stop\n",
            "Finished Traning\n",
            "10000测试图像 准确率:85.7500%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}