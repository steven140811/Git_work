{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working\n"
     ]
    }
   ],
   "source": [
    "import  os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 在Kaggle上跑的准确率为94.5400%，由于结果过长所以无法在本地显示全部记录，在kaggle上已截图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1/500] train_loss: 0.61419 valid_loss: 0.18850\n",
      "using time is :220.15036582946777\n",
      "Validation loss decreased (inf --> 0.188499).  Saving model ...\n",
      "[  2/500] train_loss: 0.12535 valid_loss: 0.16673\n",
      "using time is :219.16784572601318\n",
      "Validation loss decreased (0.188499 --> 0.166726).  Saving model ...\n",
      "[  3/500] train_loss: 0.06866 valid_loss: 0.16084\n",
      "using time is :217.00337195396423\n",
      "Validation loss decreased (0.166726 --> 0.160845).  Saving model ...\n",
      "[  4/500] train_loss: 0.05097 valid_loss: 0.15770\n",
      "using time is :217.22592043876648\n",
      "Validation loss decreased (0.160845 --> 0.157701).  Saving model ...\n",
      "[  5/500] train_loss: 0.04141 valid_loss: 0.16633\n",
      "using time is :219.93618965148926\n",
      "EarlyStopping counter: 1 out of 10\n",
      "[  6/500] train_loss: 0.03602 valid_loss: 0.16447\n",
      "using time is :219.5357780456543\n",
      "EarlyStopping counter: 2 out of 10\n",
      "[  7/500] train_loss: 0.02741 valid_loss: 0.17197\n",
      "using time is :220.04452300071716\n",
      "EarlyStopping counter: 3 out of 10\n",
      "[  8/500] train_loss: 0.02894 valid_loss: 0.18192\n",
      "using time is :219.96118593215942\n",
      "EarlyStopping counter: 4 out of 10\n",
      "[  9/500] train_loss: 0.02629 valid_loss: 0.16936\n",
      "using time is :220.61276412010193\n",
      "EarlyStopping counter: 5 out of 10\n",
      "[ 10/500] train_loss: 0.02244 valid_loss: 0.18886\n",
      "using time is :221.36105751991272\n",
      "EarlyStopping counter: 6 out of 10\n",
      "[ 11/500] train_loss: 0.02747 valid_loss: 0.17484\n",
      "using time is :220.19092965126038\n",
      "EarlyStopping counter: 7 out of 10\n",
      "[ 12/500] train_loss: 0.02503 valid_loss: 0.21423\n",
      "using time is :220.79910922050476\n",
      "EarlyStopping counter: 8 out of 10\n",
      "[ 13/500] train_loss: 0.02035 valid_loss: 0.16296\n",
      "using time is :220.6955544948578\n",
      "EarlyStopping counter: 9 out of 10\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms import ToPILImage\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "%matplotlib inline\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "show = ToPILImage() #可以把Tensor转换成Image,方便可视化\n",
    "#训练集和验证集用随机剪裁\n",
    "transform_train = transforms.Compose([  #transforms.Compose就是将对图像处理的方法集中起来\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.RandomHorizontalFlip(),#水平翻转\n",
    "    #transforms.RandomCrop((32, 32), padding=4),#对图片进行随机裁剪，裁剪的大小是32*32的，填充是4\n",
    "#     参数：size- (sequence or int)，若为sequence,则为(h,w)，若为int，则(size,size)\n",
    "#     padding-(sequence or int, optional)，此参数是设置填充多少个pixel。\n",
    "#     当为int时，图像上下左右均填充int个，例如padding=4，则上下左右均填充4个pixel，若为32x32，则会变成40x40。\n",
    "#     当为sequence时，若有2个数，则第一个数表示左右扩充多少，第二个数表示上下的。当有4个数时，则为左，上，右，下。\n",
    "#     fill- (int or tuple) 填充的值是什么（仅当填充模式为constant时有用）。int时，各通道均填充该值，当长度为3的tuple时，表示RGB通道需要填充的值。\n",
    "    transforms.ToTensor(),#转为Tensor\n",
    "    #在做数据归一化之前必须要把PIL Image转成Tensor，而其他resize或crop操作则不需要。\n",
    "     transforms.Normalize((0.5, 0.5 ,0.5), (0.5, 0.5, 0.5)),#归一化\n",
    "#     归一化操作，这里有两个参数一个是均值，一个是方差，由于是RGB型的所以一个参有三个值，这里面的参数的大小是可调的，调节的公式是：\n",
    "#     class torchvision.transforms.Normalize(mean, std)，\n",
    "#     Normalized_image=(image-mean)/std\n",
    "#     channel=（channel-mean）/std(因为transforms.ToTensor()已经把数据处理成[0,1],那么(x-0.5)/0.5\n",
    "#     就是[-1.0, 1.0])这样一来，我们的数据中的每个值就变成了[-1,1]的数了。\n",
    "    ])\n",
    "#测试集要用中心剪裁\n",
    "transform_test = transforms.Compose([  #transforms.Compose就是将对图像处理的方法集中起来\n",
    "    transforms.Resize((224,224)),\n",
    "    #transforms.RandomHorizontalFlip(),#水平翻转\n",
    "    #transforms.CenterCrop((32, 32)),#对图片进行中心裁剪，裁剪的大小是32*32的\n",
    "    transforms.ToTensor(),#转为Tensor\n",
    "    #在做数据归一化之前必须要把PIL Image转成Tensor，而其他resize或crop操作则不需要。\n",
    "     transforms.Normalize((0.5, 0.5 ,0.5), (0.5, 0.5, 0.5)),#归一化\n",
    "    ])\n",
    "\n",
    "\n",
    "\n",
    "# CIFAR-10数据集下载\n",
    "train_data = datasets.CIFAR10(\n",
    "    root='/kaggle/working',\n",
    "                         train=True,                         # 训练集\n",
    "                         # 数据变换(0, 255) -> (0, 1)\n",
    "                         transform=transform_train\n",
    "                              #download=True\n",
    "                           )\n",
    "\n",
    "test_data = datasets.CIFAR10(\n",
    "    root='/kaggle/working',\n",
    "                        train=False,                         # 测试集\n",
    "                        transform=transform_test\n",
    "                        #download=True\n",
    "                          )\n",
    "\n",
    "#划分训练集用于生成验证集\n",
    "valid_size = 0.2\n",
    "BATCH_SIZE = 160\n",
    "num_train = len(train_data)\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "#定义用于获取训练和验证批次的采样器\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "# 使用DataLoader进行分批\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, sampler=train_sampler, num_workers=0)\n",
    "valid_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, sampler=valid_sampler, num_workers=0)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=BATCH_SIZE)\n",
    "\n",
    "#数据集10个类的定义\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), 'checkpoint.pt')\t# 这里会存储迄今最优模型的参数\n",
    "        self.val_loss_min = val_loss\n",
    "\n",
    "# resnet34 Model\n",
    "model = torchvision.models.resnet34(pretrained=True)\n",
    "\n",
    "batch_size = 160\n",
    "n_epochs = 500\n",
    "\n",
    "# early stopping patience; how long to wait after last time validation loss improved.\n",
    "patience = 10\n",
    "    \n",
    "#损失函数:这里用交叉熵\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#优化器 Adam\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "#device : GPU or CPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# to track the training loss as the model trains\n",
    "train_losses = []\n",
    "# to track the validation loss as the model trains\n",
    "valid_losses = []\n",
    "# to track the average training loss per epoch as the model trains\n",
    "avg_train_losses = []\n",
    "# to track the average validation loss per epoch as the model trains\n",
    "avg_valid_losses = [] \n",
    "\n",
    "# initialize the early_stopping object\n",
    "early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
    " \n",
    "for epoch in range(1, n_epochs + 1):\n",
    "\n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    model.train() # prep model for training\n",
    "    start = time.time()\n",
    "    # for batch, (data, target) in enumerate(train_loader, 1):\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        # 前向传播\n",
    "        #print('前向传播')\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        outputs = model(inputs)\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # calculate the loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # record training loss\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "    ######################    \n",
    "    # validate the model #\n",
    "    ######################\n",
    "    model.eval() # prep model for evaluation\n",
    "    # for data, target in valid_loader:\n",
    "    for i, data in enumerate(valid_loader):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(inputs)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, labels)\n",
    "        # record validation loss\n",
    "        valid_losses.append(loss.item())\n",
    "\n",
    "    # print training/validation statistics \n",
    "    # calculate average loss over an epoch\n",
    "    train_loss = np.average(train_losses)\n",
    "    valid_loss = np.average(valid_losses)\n",
    "    avg_train_losses.append(train_loss)\n",
    "    avg_valid_losses.append(valid_loss)\n",
    "    \n",
    "    epoch_len = len(str(n_epochs))\n",
    "    \n",
    "    print_msg = (f'[{epoch:>{epoch_len}}/{n_epochs:>{epoch_len}}] ' +\n",
    "                  f'train_loss: {train_loss:.5f} ' +\n",
    "                  f'valid_loss: {valid_loss:.5f}')\n",
    "    \n",
    "    print(print_msg)\n",
    "    print('using time is :{}'.format(time.time()-start))\n",
    "    # clear lists to track next epoch\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    \n",
    "    # early_stopping needs the validation loss to check if it has decresed, \n",
    "    # and if it has, it will make a checkpoint of the current model\n",
    "    early_stopping(valid_loss, model)\n",
    "    \n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "    \n",
    "# load the last checkpoint with the best model\n",
    "model.load_state_dict(torch.load('checkpoint.pt'))\n",
    "\n",
    "\n",
    "\n",
    "# model, train_loss, valid_loss = train_model(model, batch_size, patience, n_epochs) \n",
    "\n",
    "#保存训练模型\n",
    "torch.save(model, '/kaggle/working/cifar_10_resnet34.pt')\n",
    "model = torch.load('/kaggle/working/cifar_10_resnet34.pt')\n",
    "\n",
    "# 测试\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for data in test_loader:\n",
    "    images, labels = data\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "    # 前向传播\n",
    "    out = model(images)\n",
    "    _, predicted = torch.max(out.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum().item()\n",
    "\n",
    "\n",
    "\n",
    "#输出识别准确率\n",
    "print('10000测试图像 准确率:{:.4f}%'.format(100 * correct / total)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
