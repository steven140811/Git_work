{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ali_used_car_nn.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "vf41F7TMsrVx",
        "outputId": "a600a2d5-e623-43c9-b595-59fff71144d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import os\n",
        "os.chdir(\"/content/drive/My Drive/Colab Notebooks/\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0D0nNit1Ez1",
        "outputId": "eca62011-fe9e-4d96-a3e9-1ef527f33776",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "drive.mount(\"/content/drive\", force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XmjrfsTs3iw",
        "outputId": "3f8782c6-45dc-440e-95f2-686dc26f7f83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import os\n",
        "#数据加载\n",
        "path = os.path.abspath(os.path.dirname(os.getcwd()) + os.path.sep + \".\")\n",
        "input_path = path + '/Colab Notebooks/'\n",
        "Train_data = pd.read_csv(input_path+'used_car_train_20200313.csv', sep=' ')\n",
        "Test_data = pd.read_csv(input_path+'used_car_testB_20200421.csv', sep=' ')\n",
        "\"\"\"\n",
        "—————————————————————————————————————————————以下为神经网络的数据处理—————————————————————————————————————————————\n",
        "\"\"\"\n",
        "\n",
        "# 合并方便后面的操作\n",
        "df = pd.concat([Train_data, Test_data], ignore_index=True)\n",
        "\n",
        "\n",
        "#选择需要使用的特征标签，由于nn会生成大量的特征，我们只需要保留原始特征和刻画几个明显特征即可\n",
        "feature = ['model','brand','bodyType','fuelType','kilometer','notRepairedDamage','power','regDate_month','creatDate_year','creatDate_month'\n",
        "    ,'v_0', 'v_1', 'v_2', 'v_3', 'v_4', 'v_5', 'v_6',\n",
        "       'v_7', 'v_8', 'v_9', 'v_10', 'v_11', 'v_12', 'v_13', 'v_14','car_age_day','car_age_year','regDate_year','name_count']\n",
        "\n",
        "\n",
        "#处理异常数据\n",
        "df.drop(df[df['seller'] == 1].index, inplace=True)\n",
        "#记录一下df的price\n",
        "df_copy = df\n",
        "\n",
        "df['power'][df['power']>600]=600\n",
        "\n",
        "#notRepairedDamage的值是0和1，然后为-的值设置为0.5，在将它进行标签转换，0->1;0.5->2;1->3;这样符合神经网络的特征提取，不确定值位于两个确定值的中间～\n",
        "df.replace(to_replace = '-', value = 0.5, inplace = True)\n",
        "le = LabelEncoder()\n",
        "df['notRepairedDamage'] = le.fit_transform(df['notRepairedDamage'].astype(str))\n",
        "\n",
        "#日期处理\n",
        "from datetime import datetime\n",
        "def date_process(x):\n",
        "    year = int(str(x)[:4])\n",
        "    month = int(str(x)[4:6])\n",
        "    day = int(str(x)[6:8])\n",
        "\n",
        "    if month < 1:\n",
        "        month = 1\n",
        "\n",
        "    date = datetime(year, month, day)\n",
        "    return date\n",
        "\n",
        "df['regDates'] = df['regDate'].apply(date_process)\n",
        "df['creatDates'] = df['creatDate'].apply(date_process)\n",
        "df['regDate_year'] = df['regDates'].dt.year\n",
        "df['regDate_month'] = df['regDates'].dt.month\n",
        "df['regDate_day'] = df['regDates'].dt.day\n",
        "df['creatDate_year'] = df['creatDates'].dt.year\n",
        "df['creatDate_month'] = df['creatDates'].dt.month\n",
        "df['creatDate_day'] = df['creatDates'].dt.day\n",
        "df['car_age_day'] = (df['creatDates'] - df['regDates']).dt.days\n",
        "df['car_age_year'] = round(df['car_age_day'] / 365, 1)\n",
        "\n",
        "#对name进行挖掘\n",
        "df['name_count'] = df.groupby(['name'])['SaleID'].transform('count')\n",
        "\n",
        "#填充众数\n",
        "df.fillna(df.median(),inplace= True)\n",
        "\n",
        "\n",
        "\n",
        "#特征归一化\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(df[feature].values)\n",
        "df= scaler.transform(df[feature].values)\n",
        "\n",
        "\n",
        "## 切割数据,导出数据,作为神经网络的训练数据\n",
        "output_path = path + '/Colab Notebooks/'\n",
        "nn_data = pd.DataFrame(df,columns=feature)\n",
        "nn_data['price']=np.array(df_copy['price'])\n",
        "nn_data['SaleID']=np.array(df_copy['SaleID'])\n",
        "print(nn_data.shape)\n",
        "train_num = df.shape[0]-50000\n",
        "nn_data[0:int(train_num)].to_csv(output_path+'train_nn.csv', index=0, sep=' ')\n",
        "nn_data[train_num:train_num+50000].to_csv(output_path+'test_nn.csv', index=0, sep=' ')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(199999, 31)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rq2OnLQjs3lQ",
        "outputId": "bc91b08d-9e16-4012-c4bf-3cbf48de4c61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import os\n",
        "import keras\n",
        "\"\"\"\n",
        "神经网络\n",
        "\"\"\"\n",
        "## 读取神经网络模型数据\n",
        "\n",
        "path = os.path.abspath(os.path.dirname(os.getcwd()) + os.path.sep + \".\")\n",
        "tree_data_path = path + '/Colab Notebooks/'\n",
        "Train_NN_data = pd.read_csv(tree_data_path+'train_nn.csv', sep=' ')\n",
        "Test_NN_data = pd.read_csv(tree_data_path+'test_nn.csv', sep=' ')\n",
        "\n",
        "numerical_cols = Train_NN_data.columns\n",
        "feature_cols = [col for col in numerical_cols if col not in ['price','SaleID']]\n",
        "## 提前特征列，标签列构造训练样本和测试样本\n",
        "X_data = Train_NN_data[feature_cols]\n",
        "X_test = Test_NN_data[feature_cols]\n",
        "\n",
        "\n",
        "x = np.array(X_data)\n",
        "y = np.array(Train_NN_data['price'])\n",
        "x_test = np.array(X_test)\n",
        "\n",
        "# callbacks 是在训练过程中进行监听的，因此是在fit函数中对callbacks进行添加\n",
        "# 定义一个callbacks的数组，然后在fit函数中将callbacks的数组当作参数传进去\n",
        "logdir = './callbacks'\n",
        "if not os.path.exists(logdir):\n",
        "    os.mkdir(logdir)\n",
        "logdir = os.path.join(\"callbacks\") # 好像只是在window上要添加这行，否则出错\n",
        "\n",
        "output_model_file = os.path.join(logdir,\"fashion_mnist_model.h5\")  # 输出的model文件\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.TensorBoard(logdir),\n",
        "    keras.callbacks.ModelCheckpoint(output_model_file, save_best_only=True),\n",
        "    keras.callbacks.EarlyStopping(patience=5, min_delta=1e-3),\n",
        "]\n",
        "\n",
        "\n",
        "kfolder = KFold(n_splits=5, shuffle=True, random_state=2018)\n",
        "oof_nn = np.zeros(len(x))\n",
        "predictions_nn = np.zeros(len(x_test))\n",
        "predictions_train_nn = np.zeros(len(x))\n",
        "kfold = kfolder.split(x, y)\n",
        "fold_ = 0\n",
        "for train_index, vali_index in kfold:\n",
        "    k_x_train = x[train_index]\n",
        "    k_y_train = y[train_index]\n",
        "    k_x_vali = x[vali_index]\n",
        "    k_y_vali = y[vali_index]\n",
        "\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(tf.keras.layers.Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.02)))\n",
        "    model.add(tf.keras.layers.Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.02)))\n",
        "    model.add(tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.02)))\n",
        "    model.add(tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.02)))\n",
        "    model.add(tf.keras.layers.Dense(1, kernel_regularizer=tf.keras.regularizers.l2(0.02)))\n",
        "\n",
        "    model.compile(loss='mean_absolute_error',\n",
        "                    optimizer=tf.keras.optimizers.Adam(),\n",
        "                  metrics=['mae'])\n",
        "\n",
        "    model.fit(k_x_train,k_y_train,batch_size =512,epochs=2000,validation_data=(k_x_vali, k_y_vali), callbacks=callbacks)#callbacks=callbacks,\n",
        "    oof_nn[vali_index] = model.predict(k_x_vali).reshape((model.predict(k_x_vali).shape[0],))\n",
        "    predictions_nn += model.predict(x_test).reshape((model.predict(x_test).shape[0],)) / kfolder.n_splits\n",
        "    predictions_train_nn += model.predict(x).reshape((model.predict(x).shape[0],)) / kfolder.n_splits\n",
        "\n",
        "print(\"NN score: {:<8.8f}\".format(mean_absolute_error(oof_nn, y)))\n",
        "\n",
        "output_path = path + '/Colab Notebooks/'\n",
        "# 测试集输出\n",
        "predictions = predictions_nn\n",
        "predictions[predictions < 0] = 0\n",
        "sub = pd.DataFrame()\n",
        "sub['SaleID'] = Test_NN_data.SaleID\n",
        "sub['price'] = predictions\n",
        "sub.to_csv(output_path+'nn_test.csv', index=False)\n",
        "\n",
        "# 验证集输出\n",
        "oof_nn[oof_nn < 0] = 0\n",
        "sub = pd.DataFrame()\n",
        "sub['SaleID'] = Train_NN_data.SaleID\n",
        "sub['price'] = oof_nn\n",
        "sub.to_csv(output_path+'nn_train.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2000\n",
            "  1/235 [..............................] - ETA: 0s - loss: 5521.8818 - mae: 5508.7842WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
            "Instructions for updating:\n",
            "use `tf.profiler.experimental.stop` instead.\n",
            "  2/235 [..............................] - ETA: 15s - loss: 5870.8340 - mae: 5857.8799WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0258s vs `on_train_batch_end` time: 0.1113s). Check your callbacks.\n",
            "235/235 [==============================] - 5s 19ms/step - loss: 3228.9573 - mae: 3210.4705 - val_loss: 1318.7882 - val_mae: 1285.6323\n",
            "Epoch 2/2000\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 1169.1245 - mae: 1131.1156 - val_loss: 1081.2191 - val_mae: 1040.4966\n",
            "Epoch 3/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 1017.2012 - mae: 974.7172 - val_loss: 967.2228 - val_mae: 923.3298\n",
            "Epoch 4/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 926.2065 - mae: 881.4611 - val_loss: 889.7217 - val_mae: 844.1917\n",
            "Epoch 5/2000\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 873.2534 - mae: 827.3568 - val_loss: 850.6444 - val_mae: 804.3007\n",
            "Epoch 6/2000\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 834.3711 - mae: 788.0428 - val_loss: 831.1160 - val_mae: 784.7686\n",
            "Epoch 7/2000\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 805.7097 - mae: 759.1906 - val_loss: 785.3846 - val_mae: 738.7354\n",
            "Epoch 8/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 782.6401 - mae: 735.6541 - val_loss: 767.3282 - val_mae: 720.3169\n",
            "Epoch 9/2000\n",
            "235/235 [==============================] - 4s 19ms/step - loss: 758.6132 - mae: 711.3035 - val_loss: 763.3293 - val_mae: 715.7623\n",
            "Epoch 10/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 744.8310 - mae: 697.0331 - val_loss: 763.9116 - val_mae: 715.6747\n",
            "Epoch 11/2000\n",
            "235/235 [==============================] - 5s 19ms/step - loss: 736.9880 - mae: 688.4863 - val_loss: 750.8528 - val_mae: 702.2320\n",
            "Epoch 12/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 728.5933 - mae: 679.7044 - val_loss: 742.1804 - val_mae: 692.9658\n",
            "Epoch 13/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 717.3667 - mae: 667.9579 - val_loss: 717.3792 - val_mae: 668.0612\n",
            "Epoch 14/2000\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 710.3563 - mae: 660.8660 - val_loss: 714.5052 - val_mae: 664.7781\n",
            "Epoch 15/2000\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 699.5109 - mae: 649.6802 - val_loss: 690.9930 - val_mae: 641.1345\n",
            "Epoch 16/2000\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 693.9982 - mae: 643.9952 - val_loss: 688.1956 - val_mae: 637.9717\n",
            "Epoch 17/2000\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 683.0963 - mae: 632.9250 - val_loss: 679.6413 - val_mae: 629.2748\n",
            "Epoch 18/2000\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 687.4006 - mae: 637.0789 - val_loss: 692.3151 - val_mae: 641.9247\n",
            "Epoch 19/2000\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 678.1638 - mae: 627.8053 - val_loss: 674.0908 - val_mae: 623.8281\n",
            "Epoch 20/2000\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 672.4016 - mae: 622.0870 - val_loss: 669.0345 - val_mae: 618.6985\n",
            "Epoch 21/2000\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 664.6183 - mae: 614.2050 - val_loss: 662.0934 - val_mae: 611.7816\n",
            "Epoch 22/2000\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 660.1230 - mae: 609.7854 - val_loss: 684.5029 - val_mae: 634.3688\n",
            "Epoch 23/2000\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 657.0531 - mae: 606.8015 - val_loss: 698.7911 - val_mae: 648.6165\n",
            "Epoch 24/2000\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 652.4423 - mae: 602.3606 - val_loss: 689.4923 - val_mae: 639.3712\n",
            "Epoch 25/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 647.2272 - mae: 597.1639 - val_loss: 645.7942 - val_mae: 595.7663\n",
            "Epoch 26/2000\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 643.7399 - mae: 593.8852 - val_loss: 680.9880 - val_mae: 631.3788\n",
            "Epoch 27/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 645.1567 - mae: 595.5809 - val_loss: 630.4114 - val_mae: 581.0261\n",
            "Epoch 28/2000\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 640.7626 - mae: 591.4691 - val_loss: 666.2941 - val_mae: 616.9119\n",
            "Epoch 29/2000\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 636.4922 - mae: 587.2125 - val_loss: 693.9468 - val_mae: 644.8228\n",
            "Epoch 30/2000\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 635.9972 - mae: 586.8424 - val_loss: 670.1753 - val_mae: 621.0831\n",
            "Epoch 31/2000\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 636.2716 - mae: 587.4981 - val_loss: 632.3237 - val_mae: 583.5440\n",
            "Epoch 32/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 631.9094 - mae: 583.2428 - val_loss: 618.1390 - val_mae: 569.4906\n",
            "Epoch 33/2000\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 625.2466 - mae: 576.7227 - val_loss: 622.4280 - val_mae: 573.9643\n",
            "Epoch 34/2000\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 624.0836 - mae: 575.4941 - val_loss: 620.0775 - val_mae: 571.5417\n",
            "Epoch 35/2000\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 627.8274 - mae: 579.4337 - val_loss: 638.5302 - val_mae: 590.1757\n",
            "Epoch 36/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 625.2280 - mae: 576.9077 - val_loss: 610.1402 - val_mae: 561.9212\n",
            "Epoch 37/2000\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 616.1679 - mae: 567.7756 - val_loss: 614.9997 - val_mae: 566.6667\n",
            "Epoch 38/2000\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 614.2319 - mae: 565.9279 - val_loss: 616.3397 - val_mae: 567.9316\n",
            "Epoch 39/2000\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 609.9315 - mae: 561.4330 - val_loss: 624.1273 - val_mae: 575.6980\n",
            "Epoch 40/2000\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 613.1768 - mae: 564.6653 - val_loss: 618.6977 - val_mae: 570.2009\n",
            "Epoch 41/2000\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 609.8403 - mae: 561.3633 - val_loss: 623.3737 - val_mae: 574.7781\n",
            "Epoch 1/2000\n",
            "  2/235 [..............................] - ETA: 10s - loss: 6020.7188 - mae: 6007.8164WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0179s vs `on_train_batch_end` time: 0.0775s). Check your callbacks.\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 3417.2686 - mae: 3399.6833 - val_loss: 1342.2798 - val_mae: 1308.6522\n",
            "Epoch 2/2000\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 1185.4637 - mae: 1146.5737 - val_loss: 1098.6205 - val_mae: 1056.9565\n",
            "Epoch 3/2000\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 1036.4919 - mae: 993.3759 - val_loss: 987.9108 - val_mae: 943.6538\n",
            "Epoch 4/2000\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 948.0347 - mae: 902.8098 - val_loss: 917.1411 - val_mae: 871.2241\n",
            "Epoch 5/2000\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 891.4309 - mae: 845.0765 - val_loss: 880.8489 - val_mae: 834.1951\n",
            "Epoch 6/2000\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 851.9365 - mae: 805.4977 - val_loss: 825.8795 - val_mae: 779.4066\n",
            "Epoch 7/2000\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 820.5434 - mae: 773.6934 - val_loss: 799.1211 - val_mae: 752.6487\n",
            "Epoch 8/2000\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 793.4744 - mae: 746.6481 - val_loss: 829.7567 - val_mae: 782.4641\n",
            "Epoch 9/2000\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 778.0931 - mae: 730.6495 - val_loss: 768.0614 - val_mae: 720.1910\n",
            "Epoch 10/2000\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 756.5453 - mae: 708.4447 - val_loss: 743.0786 - val_mae: 694.4145\n",
            "Epoch 11/2000\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 745.2422 - mae: 696.3013 - val_loss: 774.8964 - val_mae: 725.7185\n",
            "Epoch 12/2000\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 727.7999 - mae: 678.3372 - val_loss: 711.7119 - val_mae: 661.8507\n",
            "Epoch 13/2000\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 713.4412 - mae: 663.3801 - val_loss: 717.6147 - val_mae: 667.3998\n",
            "Epoch 14/2000\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 711.6650 - mae: 661.2783 - val_loss: 690.9882 - val_mae: 640.4340\n",
            "Epoch 15/2000\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 696.8524 - mae: 645.7977 - val_loss: 684.8401 - val_mae: 633.8932\n",
            "Epoch 16/2000\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 685.5363 - mae: 634.2687 - val_loss: 681.7279 - val_mae: 630.4070\n",
            "Epoch 17/2000\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 682.0596 - mae: 630.6179 - val_loss: 665.5955 - val_mae: 614.1889\n",
            "Epoch 18/2000\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 672.2608 - mae: 620.7339 - val_loss: 691.7187 - val_mae: 639.9807\n",
            "Epoch 19/2000\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 668.8096 - mae: 617.1320 - val_loss: 649.5733 - val_mae: 597.9603\n",
            "Epoch 20/2000\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 660.8237 - mae: 609.3011 - val_loss: 644.6187 - val_mae: 592.7734\n",
            "Epoch 21/2000\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 658.3693 - mae: 606.7346 - val_loss: 643.8033 - val_mae: 592.2726\n",
            "Epoch 22/2000\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 646.5183 - mae: 594.9719 - val_loss: 637.4338 - val_mae: 585.9797\n",
            "Epoch 23/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 643.9849 - mae: 592.5597 - val_loss: 624.9293 - val_mae: 573.5766\n",
            "Epoch 24/2000\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 646.2003 - mae: 594.9689 - val_loss: 657.7567 - val_mae: 606.4318\n",
            "Epoch 25/2000\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 636.8331 - mae: 585.5382 - val_loss: 654.9535 - val_mae: 603.5514\n",
            "Epoch 26/2000\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 634.2901 - mae: 582.9096 - val_loss: 613.7870 - val_mae: 562.3026\n",
            "Epoch 27/2000\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 623.8460 - mae: 572.3309 - val_loss: 625.1960 - val_mae: 573.3785\n",
            "Epoch 28/2000\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 624.6710 - mae: 572.9625 - val_loss: 615.9305 - val_mae: 564.2919\n",
            "Epoch 29/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 623.3587 - mae: 571.7612 - val_loss: 609.7751 - val_mae: 558.2311\n",
            "Epoch 30/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 617.7523 - mae: 566.2773 - val_loss: 613.1667 - val_mae: 561.6788\n",
            "Epoch 31/2000\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 615.6956 - mae: 564.1194 - val_loss: 606.3495 - val_mae: 554.9177\n",
            "Epoch 32/2000\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 612.4164 - mae: 561.0128 - val_loss: 609.8917 - val_mae: 558.5814\n",
            "Epoch 33/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 616.2035 - mae: 565.0660 - val_loss: 596.7416 - val_mae: 545.6068\n",
            "Epoch 34/2000\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 610.8297 - mae: 559.6563 - val_loss: 597.4196 - val_mae: 546.1215\n",
            "Epoch 35/2000\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 609.9683 - mae: 558.7408 - val_loss: 597.3271 - val_mae: 546.0349\n",
            "Epoch 36/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 606.3259 - mae: 555.2092 - val_loss: 591.4926 - val_mae: 540.4680\n",
            "Epoch 37/2000\n",
            "235/235 [==============================] - 7s 30ms/step - loss: 599.4673 - mae: 548.4790 - val_loss: 605.5669 - val_mae: 554.7018\n",
            "Epoch 38/2000\n",
            "235/235 [==============================] - 6s 24ms/step - loss: 602.7577 - mae: 551.8207 - val_loss: 586.6417 - val_mae: 535.8275\n",
            "Epoch 39/2000\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 597.2420 - mae: 546.3140 - val_loss: 602.6405 - val_mae: 551.6531\n",
            "Epoch 40/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 595.7279 - mae: 544.8027 - val_loss: 598.1924 - val_mae: 547.2573\n",
            "Epoch 41/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 593.2973 - mae: 542.2666 - val_loss: 594.7127 - val_mae: 543.7107\n",
            "Epoch 42/2000\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 588.4398 - mae: 537.3411 - val_loss: 576.6451 - val_mae: 525.6410\n",
            "Epoch 43/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 593.2413 - mae: 542.3373 - val_loss: 590.5399 - val_mae: 539.3381\n",
            "Epoch 44/2000\n",
            "235/235 [==============================] - 4s 19ms/step - loss: 583.5082 - mae: 532.4474 - val_loss: 581.7241 - val_mae: 530.7597\n",
            "Epoch 45/2000\n",
            "235/235 [==============================] - 4s 19ms/step - loss: 587.4324 - mae: 536.5091 - val_loss: 581.2967 - val_mae: 530.4792\n",
            "Epoch 46/2000\n",
            "235/235 [==============================] - 4s 19ms/step - loss: 584.5517 - mae: 533.5777 - val_loss: 574.2428 - val_mae: 523.3869\n",
            "Epoch 47/2000\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 588.4876 - mae: 537.5570 - val_loss: 574.6364 - val_mae: 523.7773\n",
            "Epoch 48/2000\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 582.9883 - mae: 532.2228 - val_loss: 576.2668 - val_mae: 525.4333\n",
            "Epoch 49/2000\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 587.1931 - mae: 536.3889 - val_loss: 608.8129 - val_mae: 558.1321\n",
            "Epoch 50/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 586.3956 - mae: 535.6430 - val_loss: 569.6943 - val_mae: 519.1434\n",
            "Epoch 51/2000\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 581.3330 - mae: 530.7745 - val_loss: 576.6696 - val_mae: 526.2913\n",
            "Epoch 52/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 576.2114 - mae: 525.7199 - val_loss: 580.7819 - val_mae: 530.1094\n",
            "Epoch 53/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 576.8021 - mae: 526.3940 - val_loss: 564.9610 - val_mae: 514.5472\n",
            "Epoch 54/2000\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 584.7736 - mae: 534.4221 - val_loss: 570.3708 - val_mae: 520.0884\n",
            "Epoch 55/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 572.3455 - mae: 522.0175 - val_loss: 584.5862 - val_mae: 534.1674\n",
            "Epoch 56/2000\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 576.4740 - mae: 526.1852 - val_loss: 570.2425 - val_mae: 520.0344\n",
            "Epoch 57/2000\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 576.2428 - mae: 526.0443 - val_loss: 575.9398 - val_mae: 525.8270\n",
            "Epoch 58/2000\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 568.4611 - mae: 518.3861 - val_loss: 574.1710 - val_mae: 524.2057\n",
            "Epoch 1/2000\n",
            "  2/235 [..............................] - ETA: 12s - loss: 6265.7070 - mae: 6252.7578WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0218s vs `on_train_batch_end` time: 0.0868s). Check your callbacks.\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 3229.3657 - mae: 3210.5991 - val_loss: 1320.7618 - val_mae: 1287.4677\n",
            "Epoch 2/2000\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 1187.7776 - mae: 1149.4103 - val_loss: 1103.9476 - val_mae: 1062.2776\n",
            "Epoch 3/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 1039.3998 - mae: 996.7244 - val_loss: 972.3590 - val_mae: 928.3707\n",
            "Epoch 4/2000\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 952.1148 - mae: 907.4265 - val_loss: 908.9352 - val_mae: 864.0579\n",
            "Epoch 5/2000\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 894.6180 - mae: 849.2620 - val_loss: 862.0792 - val_mae: 816.5587\n",
            "Epoch 6/2000\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 855.6395 - mae: 810.0090 - val_loss: 820.2421 - val_mae: 774.1190\n",
            "Epoch 7/2000\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 818.7235 - mae: 772.3100 - val_loss: 850.1755 - val_mae: 803.7269\n",
            "Epoch 8/2000\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 799.3631 - mae: 752.5504 - val_loss: 766.4330 - val_mae: 719.3102\n",
            "Epoch 9/2000\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 772.5551 - mae: 725.0634 - val_loss: 750.8383 - val_mae: 703.2325\n",
            "Epoch 10/2000\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 756.5176 - mae: 708.5260 - val_loss: 731.9688 - val_mae: 683.6951\n",
            "Epoch 11/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 745.6000 - mae: 697.0994 - val_loss: 725.3801 - val_mae: 676.6436\n",
            "Epoch 12/2000\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 735.5418 - mae: 686.5508 - val_loss: 713.0900 - val_mae: 664.0353\n",
            "Epoch 13/2000\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 726.3055 - mae: 677.0399 - val_loss: 708.3749 - val_mae: 658.8127\n",
            "Epoch 14/2000\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 726.8033 - mae: 677.1682 - val_loss: 735.7342 - val_mae: 686.1349\n",
            "Epoch 15/2000\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 712.7946 - mae: 662.9783 - val_loss: 704.5306 - val_mae: 654.5889\n",
            "Epoch 16/2000\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 707.1871 - mae: 657.1486 - val_loss: 765.9354 - val_mae: 715.9834\n",
            "Epoch 17/2000\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 711.0697 - mae: 661.0450 - val_loss: 697.4857 - val_mae: 647.2542\n",
            "Epoch 18/2000\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 702.0924 - mae: 651.9775 - val_loss: 684.2065 - val_mae: 633.8192\n",
            "Epoch 19/2000\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 694.8757 - mae: 644.5309 - val_loss: 693.1174 - val_mae: 642.7085\n",
            "Epoch 20/2000\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 690.9178 - mae: 640.6797 - val_loss: 672.9587 - val_mae: 622.6345\n",
            "Epoch 21/2000\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 688.8842 - mae: 638.6765 - val_loss: 672.1740 - val_mae: 622.0147\n",
            "Epoch 22/2000\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 687.9619 - mae: 637.8914 - val_loss: 673.8322 - val_mae: 623.9431\n",
            "Epoch 23/2000\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 676.9615 - mae: 626.9576 - val_loss: 658.6438 - val_mae: 608.5136\n",
            "Epoch 24/2000\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 671.2708 - mae: 621.1980 - val_loss: 664.3640 - val_mae: 614.2874\n",
            "Epoch 25/2000\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 669.9116 - mae: 619.9535 - val_loss: 662.9362 - val_mae: 613.0911\n",
            "Epoch 26/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 665.3250 - mae: 615.4529 - val_loss: 653.0057 - val_mae: 603.2896\n",
            "Epoch 27/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 663.9387 - mae: 614.4092 - val_loss: 643.4755 - val_mae: 593.9946\n",
            "Epoch 28/2000\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 654.5266 - mae: 604.9863 - val_loss: 656.7330 - val_mae: 607.4330\n",
            "Epoch 29/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 660.0028 - mae: 610.6784 - val_loss: 632.0377 - val_mae: 582.6428\n",
            "Epoch 30/2000\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 637.6646 - mae: 588.2836 - val_loss: 643.8831 - val_mae: 594.4912\n",
            "Epoch 31/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 638.4825 - mae: 589.0020 - val_loss: 631.7098 - val_mae: 582.1959\n",
            "Epoch 32/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 630.4647 - mae: 580.9938 - val_loss: 647.5406 - val_mae: 597.7554\n",
            "Epoch 33/2000\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 627.3886 - mae: 577.7457 - val_loss: 611.6414 - val_mae: 562.1953\n",
            "Epoch 34/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 625.6364 - mae: 576.0685 - val_loss: 640.5599 - val_mae: 590.9114\n",
            "Epoch 35/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 620.2202 - mae: 570.3546 - val_loss: 624.1095 - val_mae: 574.5369\n",
            "Epoch 36/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 617.4354 - mae: 567.7722 - val_loss: 633.8030 - val_mae: 584.2645\n",
            "Epoch 37/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 611.7070 - mae: 562.0859 - val_loss: 602.8818 - val_mae: 553.2900\n",
            "Epoch 38/2000\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 615.4637 - mae: 565.8841 - val_loss: 600.8211 - val_mae: 551.5092\n",
            "Epoch 39/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 620.6003 - mae: 571.3004 - val_loss: 648.2385 - val_mae: 599.1720\n",
            "Epoch 40/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 612.7648 - mae: 563.7188 - val_loss: 617.6929 - val_mae: 568.4412\n",
            "Epoch 41/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 608.8929 - mae: 559.7257 - val_loss: 615.4589 - val_mae: 566.4015\n",
            "Epoch 42/2000\n",
            "235/235 [==============================] - 4s 19ms/step - loss: 600.6912 - mae: 551.7841 - val_loss: 598.6504 - val_mae: 549.8405\n",
            "Epoch 43/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 605.0614 - mae: 556.1653 - val_loss: 590.3845 - val_mae: 541.6922\n",
            "Epoch 44/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 600.2151 - mae: 551.4217 - val_loss: 605.9927 - val_mae: 557.3805\n",
            "Epoch 45/2000\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 597.2227 - mae: 548.6807 - val_loss: 595.2205 - val_mae: 546.6740\n",
            "Epoch 46/2000\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 601.1062 - mae: 552.3721 - val_loss: 605.6055 - val_mae: 557.0806\n",
            "Epoch 47/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 601.1424 - mae: 552.7772 - val_loss: 590.5737 - val_mae: 542.1956\n",
            "Epoch 48/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 599.0916 - mae: 550.6966 - val_loss: 605.3057 - val_mae: 556.8400\n",
            "Epoch 1/2000\n",
            "  2/235 [..............................] - ETA: 14s - loss: 5568.2197 - mae: 5555.2754WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0203s vs `on_train_batch_end` time: 0.1071s). Check your callbacks.\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 3655.2285 - mae: 3636.6089 - val_loss: 1413.4926 - val_mae: 1373.8069\n",
            "Epoch 2/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 1245.4722 - mae: 1198.6870 - val_loss: 1138.4100 - val_mae: 1087.7231\n",
            "Epoch 3/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 1094.2631 - mae: 1042.5544 - val_loss: 1072.8428 - val_mae: 1020.6374\n",
            "Epoch 4/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 1005.9672 - mae: 953.3283 - val_loss: 958.5530 - val_mae: 905.6377\n",
            "Epoch 5/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 940.7719 - mae: 887.9299 - val_loss: 919.7727 - val_mae: 866.9214\n",
            "Epoch 6/2000\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 894.4842 - mae: 841.5784 - val_loss: 928.1797 - val_mae: 875.3124\n",
            "Epoch 7/2000\n",
            "235/235 [==============================] - 5s 19ms/step - loss: 858.4092 - mae: 805.2618 - val_loss: 836.4483 - val_mae: 783.4207\n",
            "Epoch 8/2000\n",
            "235/235 [==============================] - 4s 19ms/step - loss: 825.3693 - mae: 772.4065 - val_loss: 824.7058 - val_mae: 771.3830\n",
            "Epoch 9/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 801.1197 - mae: 747.7498 - val_loss: 854.9960 - val_mae: 801.4851\n",
            "Epoch 10/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 780.9457 - mae: 727.0701 - val_loss: 770.9030 - val_mae: 716.8080\n",
            "Epoch 11/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 762.1173 - mae: 707.8413 - val_loss: 754.2928 - val_mae: 699.7658\n",
            "Epoch 12/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 745.7201 - mae: 691.0823 - val_loss: 736.8258 - val_mae: 682.0273\n",
            "Epoch 13/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 736.8029 - mae: 681.7665 - val_loss: 725.5659 - val_mae: 670.3959\n",
            "Epoch 14/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 724.5641 - mae: 669.1884 - val_loss: 715.8256 - val_mae: 659.9738\n",
            "Epoch 15/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 714.6949 - mae: 658.8329 - val_loss: 747.2447 - val_mae: 691.3698\n",
            "Epoch 16/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 707.8559 - mae: 651.8190 - val_loss: 699.5761 - val_mae: 643.4509\n",
            "Epoch 17/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 697.2881 - mae: 641.3027 - val_loss: 693.3312 - val_mae: 637.5367\n",
            "Epoch 18/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 688.8821 - mae: 632.9577 - val_loss: 684.0869 - val_mae: 628.0564\n",
            "Epoch 19/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 682.7008 - mae: 626.8421 - val_loss: 682.4969 - val_mae: 626.3237\n",
            "Epoch 20/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 682.6614 - mae: 626.6008 - val_loss: 762.0800 - val_mae: 706.0609\n",
            "Epoch 21/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 676.8952 - mae: 621.0112 - val_loss: 694.5901 - val_mae: 638.8262\n",
            "Epoch 22/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 671.2139 - mae: 615.4464 - val_loss: 666.5293 - val_mae: 610.5614\n",
            "Epoch 23/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 662.5861 - mae: 606.7749 - val_loss: 686.3027 - val_mae: 630.6185\n",
            "Epoch 24/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 658.0057 - mae: 602.3991 - val_loss: 656.7478 - val_mae: 601.2932\n",
            "Epoch 25/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 654.7611 - mae: 599.3007 - val_loss: 685.7844 - val_mae: 630.5940\n",
            "Epoch 26/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 653.6537 - mae: 598.5625 - val_loss: 644.4902 - val_mae: 589.3480\n",
            "Epoch 27/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 651.8560 - mae: 596.8210 - val_loss: 661.9471 - val_mae: 607.0526\n",
            "Epoch 28/2000\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 645.6473 - mae: 590.9542 - val_loss: 641.1180 - val_mae: 586.4464\n",
            "Epoch 29/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 640.2723 - mae: 585.6854 - val_loss: 666.4409 - val_mae: 611.8004\n",
            "Epoch 30/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 639.5566 - mae: 585.3162 - val_loss: 654.9393 - val_mae: 600.8809\n",
            "Epoch 31/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 638.5956 - mae: 584.5123 - val_loss: 703.0835 - val_mae: 649.2984\n",
            "Epoch 32/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 633.2538 - mae: 579.7362 - val_loss: 634.4649 - val_mae: 580.9113\n",
            "Epoch 33/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 630.2201 - mae: 576.8456 - val_loss: 640.4213 - val_mae: 587.1779\n",
            "Epoch 34/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 626.0652 - mae: 573.0296 - val_loss: 628.4550 - val_mae: 575.6003\n",
            "Epoch 35/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 627.9734 - mae: 574.9993 - val_loss: 629.8477 - val_mae: 577.3191\n",
            "Epoch 36/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 623.0861 - mae: 570.4676 - val_loss: 624.2349 - val_mae: 571.6809\n",
            "Epoch 37/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 618.2466 - mae: 565.6969 - val_loss: 614.7698 - val_mae: 562.0980\n",
            "Epoch 38/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 613.9482 - mae: 561.2525 - val_loss: 610.3932 - val_mae: 558.1169\n",
            "Epoch 39/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 620.6584 - mae: 568.2977 - val_loss: 610.9297 - val_mae: 558.5003\n",
            "Epoch 40/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 614.5775 - mae: 561.9874 - val_loss: 643.2775 - val_mae: 590.5728\n",
            "Epoch 41/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 603.1635 - mae: 550.3016 - val_loss: 602.7984 - val_mae: 549.7809\n",
            "Epoch 42/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 607.6523 - mae: 554.6232 - val_loss: 605.0450 - val_mae: 552.1376\n",
            "Epoch 43/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 602.3724 - mae: 549.4734 - val_loss: 658.2398 - val_mae: 605.4449\n",
            "Epoch 44/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 603.3813 - mae: 550.5544 - val_loss: 605.4161 - val_mae: 552.6000\n",
            "Epoch 45/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 597.1453 - mae: 544.2252 - val_loss: 594.2102 - val_mae: 541.3500\n",
            "Epoch 46/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 599.7433 - mae: 546.7398 - val_loss: 600.6815 - val_mae: 547.6897\n",
            "Epoch 47/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 596.5971 - mae: 543.6461 - val_loss: 596.6342 - val_mae: 543.6979\n",
            "Epoch 48/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 596.4389 - mae: 543.5106 - val_loss: 597.1903 - val_mae: 544.3891\n",
            "Epoch 49/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 589.3125 - mae: 536.5471 - val_loss: 589.6151 - val_mae: 536.9515\n",
            "Epoch 50/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 593.3533 - mae: 540.6780 - val_loss: 586.1396 - val_mae: 533.3298\n",
            "Epoch 51/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 584.6099 - mae: 532.0250 - val_loss: 587.0490 - val_mae: 534.5098\n",
            "Epoch 52/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 593.7768 - mae: 541.1957 - val_loss: 600.0987 - val_mae: 547.7171\n",
            "Epoch 53/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 588.5040 - mae: 536.1871 - val_loss: 585.2676 - val_mae: 532.9493\n",
            "Epoch 54/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 586.3215 - mae: 534.0167 - val_loss: 579.8246 - val_mae: 527.6177\n",
            "Epoch 55/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 585.3063 - mae: 533.2327 - val_loss: 580.3382 - val_mae: 528.2191\n",
            "Epoch 56/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 583.0928 - mae: 530.8031 - val_loss: 597.3217 - val_mae: 545.2740\n",
            "Epoch 57/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 577.0687 - mae: 524.9653 - val_loss: 578.4858 - val_mae: 526.7587\n",
            "Epoch 58/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 581.0005 - mae: 529.1483 - val_loss: 580.6351 - val_mae: 528.8890\n",
            "Epoch 59/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 581.0187 - mae: 529.2827 - val_loss: 578.6649 - val_mae: 527.2693\n",
            "Epoch 60/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 581.4191 - mae: 529.9632 - val_loss: 601.3887 - val_mae: 549.9948\n",
            "Epoch 61/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 579.4202 - mae: 528.1624 - val_loss: 583.6403 - val_mae: 532.4011\n",
            "Epoch 62/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 575.0716 - mae: 523.9183 - val_loss: 576.0951 - val_mae: 524.8821\n",
            "Epoch 63/2000\n",
            "235/235 [==============================] - 5s 19ms/step - loss: 581.9986 - mae: 530.9171 - val_loss: 654.6722 - val_mae: 603.7449\n",
            "Epoch 64/2000\n",
            "235/235 [==============================] - 4s 19ms/step - loss: 577.5419 - mae: 526.5707 - val_loss: 602.1373 - val_mae: 551.5474\n",
            "Epoch 65/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 578.5853 - mae: 527.8813 - val_loss: 572.6232 - val_mae: 521.9329\n",
            "Epoch 66/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 572.1779 - mae: 521.6481 - val_loss: 575.9404 - val_mae: 525.4493\n",
            "Epoch 67/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 575.5618 - mae: 525.0672 - val_loss: 591.0950 - val_mae: 540.8589\n",
            "Epoch 68/2000\n",
            "235/235 [==============================] - 7s 28ms/step - loss: 573.6078 - mae: 523.3585 - val_loss: 577.3908 - val_mae: 527.2314\n",
            "Epoch 69/2000\n",
            "235/235 [==============================] - 7s 28ms/step - loss: 568.4523 - mae: 518.3455 - val_loss: 604.3021 - val_mae: 554.4402\n",
            "Epoch 70/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 570.9153 - mae: 520.9494 - val_loss: 574.2808 - val_mae: 524.4210\n",
            "Epoch 1/2000\n",
            "  2/235 [..............................] - ETA: 13s - loss: 5843.4883 - mae: 5830.5757WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0201s vs `on_train_batch_end` time: 0.0976s). Check your callbacks.\n",
            "235/235 [==============================] - 4s 19ms/step - loss: 3306.1616 - mae: 3288.4058 - val_loss: 1351.4777 - val_mae: 1318.3442\n",
            "Epoch 2/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 1183.5358 - mae: 1145.5140 - val_loss: 1126.0439 - val_mae: 1085.0922\n",
            "Epoch 3/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 1043.7668 - mae: 1001.5549 - val_loss: 1005.4944 - val_mae: 962.3506\n",
            "Epoch 4/2000\n",
            "235/235 [==============================] - 4s 19ms/step - loss: 951.1895 - mae: 906.8914 - val_loss: 925.5326 - val_mae: 880.8308\n",
            "Epoch 5/2000\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 892.6950 - mae: 847.2044 - val_loss: 871.1867 - val_mae: 825.3082\n",
            "Epoch 6/2000\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 852.1193 - mae: 806.0768 - val_loss: 852.7287 - val_mae: 806.5172\n",
            "Epoch 7/2000\n",
            "235/235 [==============================] - 5s 19ms/step - loss: 822.7485 - mae: 776.2427 - val_loss: 806.1075 - val_mae: 759.3978\n",
            "Epoch 8/2000\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 796.3824 - mae: 749.1840 - val_loss: 779.1926 - val_mae: 732.0445\n",
            "Epoch 9/2000\n",
            "235/235 [==============================] - 5s 19ms/step - loss: 775.4750 - mae: 727.9343 - val_loss: 766.5426 - val_mae: 718.8723\n",
            "Epoch 10/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 759.4286 - mae: 711.4349 - val_loss: 749.6105 - val_mae: 701.2479\n",
            "Epoch 11/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 742.4385 - mae: 693.7351 - val_loss: 752.4913 - val_mae: 703.7260\n",
            "Epoch 12/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 732.9017 - mae: 683.7844 - val_loss: 745.5261 - val_mae: 696.3267\n",
            "Epoch 13/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 730.1956 - mae: 680.8245 - val_loss: 730.8980 - val_mae: 681.5347\n",
            "Epoch 14/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 715.2269 - mae: 665.6822 - val_loss: 705.7917 - val_mae: 655.8459\n",
            "Epoch 15/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 713.0601 - mae: 663.1039 - val_loss: 713.6007 - val_mae: 663.7330\n",
            "Epoch 16/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 698.2759 - mae: 648.3256 - val_loss: 748.0391 - val_mae: 697.8120\n",
            "Epoch 17/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 697.3289 - mae: 646.9388 - val_loss: 698.6589 - val_mae: 648.3372\n",
            "Epoch 18/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 688.6948 - mae: 638.1838 - val_loss: 684.0587 - val_mae: 633.7655\n",
            "Epoch 19/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 682.6201 - mae: 632.1699 - val_loss: 680.9669 - val_mae: 630.5802\n",
            "Epoch 20/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 678.0955 - mae: 627.7461 - val_loss: 699.7595 - val_mae: 649.2664\n",
            "Epoch 21/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 676.3326 - mae: 625.8026 - val_loss: 672.9042 - val_mae: 622.3424\n",
            "Epoch 22/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 668.5120 - mae: 618.0534 - val_loss: 678.8526 - val_mae: 628.3116\n",
            "Epoch 23/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 663.8635 - mae: 613.4015 - val_loss: 657.8243 - val_mae: 607.3278\n",
            "Epoch 24/2000\n",
            "235/235 [==============================] - 4s 19ms/step - loss: 655.4893 - mae: 605.0283 - val_loss: 703.7578 - val_mae: 653.5452\n",
            "Epoch 25/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 656.1832 - mae: 605.9086 - val_loss: 654.8662 - val_mae: 604.6005\n",
            "Epoch 26/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 650.6876 - mae: 600.4403 - val_loss: 653.4174 - val_mae: 603.2421\n",
            "Epoch 27/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 645.0428 - mae: 595.2900 - val_loss: 659.2050 - val_mae: 609.5567\n",
            "Epoch 28/2000\n",
            "235/235 [==============================] - 4s 19ms/step - loss: 643.4423 - mae: 593.5774 - val_loss: 643.3044 - val_mae: 593.6479\n",
            "Epoch 29/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 638.2410 - mae: 588.6161 - val_loss: 647.0829 - val_mae: 597.7536\n",
            "Epoch 30/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 636.4008 - mae: 587.0761 - val_loss: 635.2225 - val_mae: 586.1537\n",
            "Epoch 31/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 637.5331 - mae: 588.4821 - val_loss: 640.3162 - val_mae: 591.5657\n",
            "Epoch 32/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 631.6684 - mae: 582.9897 - val_loss: 642.2527 - val_mae: 593.4761\n",
            "Epoch 33/2000\n",
            "235/235 [==============================] - 4s 19ms/step - loss: 628.6287 - mae: 580.0812 - val_loss: 642.3808 - val_mae: 594.1753\n",
            "Epoch 34/2000\n",
            "235/235 [==============================] - 4s 19ms/step - loss: 630.3688 - mae: 582.1263 - val_loss: 629.4025 - val_mae: 581.1340\n",
            "Epoch 35/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 627.7302 - mae: 579.6324 - val_loss: 644.8918 - val_mae: 596.9949\n",
            "Epoch 36/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 629.9432 - mae: 582.0359 - val_loss: 636.2641 - val_mae: 588.4391\n",
            "Epoch 37/2000\n",
            "235/235 [==============================] - 4s 19ms/step - loss: 622.6061 - mae: 574.8055 - val_loss: 670.0778 - val_mae: 622.3467\n",
            "Epoch 38/2000\n",
            "235/235 [==============================] - 4s 19ms/step - loss: 625.3546 - mae: 577.6000 - val_loss: 620.7047 - val_mae: 573.3340\n",
            "Epoch 39/2000\n",
            "235/235 [==============================] - 4s 19ms/step - loss: 616.5844 - mae: 569.3505 - val_loss: 643.5154 - val_mae: 596.3142\n",
            "Epoch 40/2000\n",
            "235/235 [==============================] - 4s 19ms/step - loss: 619.3516 - mae: 572.0125 - val_loss: 634.5685 - val_mae: 587.2292\n",
            "Epoch 41/2000\n",
            "235/235 [==============================] - 4s 19ms/step - loss: 613.7214 - mae: 566.4222 - val_loss: 642.7408 - val_mae: 595.5032\n",
            "Epoch 42/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 608.7390 - mae: 561.4826 - val_loss: 615.6365 - val_mae: 568.2064\n",
            "Epoch 43/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 602.3711 - mae: 554.9184 - val_loss: 609.6239 - val_mae: 562.0081\n",
            "Epoch 44/2000\n",
            "235/235 [==============================] - 4s 19ms/step - loss: 604.3575 - mae: 556.7925 - val_loss: 604.1919 - val_mae: 556.6184\n",
            "Epoch 45/2000\n",
            "235/235 [==============================] - 4s 19ms/step - loss: 598.0359 - mae: 550.4135 - val_loss: 633.5445 - val_mae: 585.6782\n",
            "Epoch 46/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 599.5565 - mae: 551.7471 - val_loss: 600.3149 - val_mae: 552.4282\n",
            "Epoch 47/2000\n",
            "235/235 [==============================] - 4s 19ms/step - loss: 600.9211 - mae: 553.0957 - val_loss: 598.2576 - val_mae: 550.2411\n",
            "Epoch 48/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 594.5052 - mae: 546.3842 - val_loss: 614.1326 - val_mae: 565.9797\n",
            "Epoch 49/2000\n",
            "235/235 [==============================] - 4s 19ms/step - loss: 588.9221 - mae: 540.7516 - val_loss: 593.5220 - val_mae: 545.4598\n",
            "Epoch 50/2000\n",
            "235/235 [==============================] - 4s 19ms/step - loss: 587.4760 - mae: 539.3265 - val_loss: 589.7010 - val_mae: 541.3631\n",
            "Epoch 51/2000\n",
            "235/235 [==============================] - 4s 19ms/step - loss: 590.6613 - mae: 542.4613 - val_loss: 597.7739 - val_mae: 549.6261\n",
            "Epoch 52/2000\n",
            "235/235 [==============================] - 4s 19ms/step - loss: 584.5020 - mae: 536.2803 - val_loss: 594.4803 - val_mae: 546.0338\n",
            "Epoch 53/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 587.5707 - mae: 539.1691 - val_loss: 605.0467 - val_mae: 556.8229\n",
            "Epoch 54/2000\n",
            "235/235 [==============================] - 4s 19ms/step - loss: 581.3220 - mae: 532.9893 - val_loss: 580.9667 - val_mae: 532.5293\n",
            "Epoch 55/2000\n",
            "235/235 [==============================] - 4s 19ms/step - loss: 578.0084 - mae: 529.6389 - val_loss: 597.2982 - val_mae: 549.0816\n",
            "Epoch 56/2000\n",
            "235/235 [==============================] - 4s 19ms/step - loss: 582.9056 - mae: 534.5915 - val_loss: 580.7866 - val_mae: 532.4838\n",
            "Epoch 57/2000\n",
            "235/235 [==============================] - 4s 19ms/step - loss: 574.3149 - mae: 525.9409 - val_loss: 605.7053 - val_mae: 557.5037\n",
            "Epoch 58/2000\n",
            "235/235 [==============================] - 4s 19ms/step - loss: 573.8852 - mae: 525.7010 - val_loss: 601.1274 - val_mae: 552.9514\n",
            "Epoch 59/2000\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 575.1453 - mae: 526.9958 - val_loss: 576.1810 - val_mae: 528.0289\n",
            "Epoch 60/2000\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 569.7928 - mae: 521.6921 - val_loss: 575.3173 - val_mae: 527.1691\n",
            "Epoch 61/2000\n",
            "235/235 [==============================] - 4s 19ms/step - loss: 570.4307 - mae: 522.2664 - val_loss: 576.7142 - val_mae: 528.6331\n",
            "Epoch 62/2000\n",
            "235/235 [==============================] - 4s 19ms/step - loss: 569.7509 - mae: 521.6833 - val_loss: 575.1519 - val_mae: 527.2233\n",
            "Epoch 63/2000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 572.2201 - mae: 524.2530 - val_loss: 570.7706 - val_mae: 522.7643\n",
            "Epoch 64/2000\n",
            "235/235 [==============================] - 4s 19ms/step - loss: 571.4201 - mae: 523.4717 - val_loss: 581.7546 - val_mae: 533.7791\n",
            "Epoch 65/2000\n",
            "235/235 [==============================] - 4s 19ms/step - loss: 568.2743 - mae: 520.2494 - val_loss: 610.2140 - val_mae: 562.4609\n",
            "Epoch 66/2000\n",
            "235/235 [==============================] - 4s 19ms/step - loss: 567.9949 - mae: 520.3167 - val_loss: 581.8596 - val_mae: 534.0557\n",
            "Epoch 67/2000\n",
            "235/235 [==============================] - 4s 19ms/step - loss: 567.5599 - mae: 519.7352 - val_loss: 567.9482 - val_mae: 520.3757\n",
            "Epoch 68/2000\n",
            "235/235 [==============================] - 4s 19ms/step - loss: 559.8090 - mae: 512.1575 - val_loss: 580.9471 - val_mae: 533.2280\n",
            "Epoch 69/2000\n",
            "235/235 [==============================] - 4s 19ms/step - loss: 566.4301 - mae: 518.9122 - val_loss: 567.9051 - val_mae: 520.3723\n",
            "Epoch 70/2000\n",
            "235/235 [==============================] - 4s 19ms/step - loss: 563.7285 - mae: 516.3306 - val_loss: 568.3093 - val_mae: 521.1953\n",
            "Epoch 71/2000\n",
            "235/235 [==============================] - 4s 19ms/step - loss: 559.9177 - mae: 512.6430 - val_loss: 615.1460 - val_mae: 567.8317\n",
            "Epoch 72/2000\n",
            "235/235 [==============================] - 4s 19ms/step - loss: 565.8509 - mae: 518.6794 - val_loss: 568.1400 - val_mae: 520.9236\n",
            "Epoch 73/2000\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 559.3287 - mae: 512.0788 - val_loss: 562.7280 - val_mae: 515.6344\n",
            "Epoch 74/2000\n",
            "235/235 [==============================] - 4s 19ms/step - loss: 560.1943 - mae: 513.0422 - val_loss: 571.9774 - val_mae: 524.8832\n",
            "Epoch 75/2000\n",
            "235/235 [==============================] - 4s 19ms/step - loss: 559.4347 - mae: 512.3558 - val_loss: 564.8925 - val_mae: 517.7726\n",
            "Epoch 76/2000\n",
            "235/235 [==============================] - 5s 19ms/step - loss: 563.7021 - mae: 516.6131 - val_loss: 564.8860 - val_mae: 518.0529\n",
            "Epoch 77/2000\n",
            "235/235 [==============================] - 4s 19ms/step - loss: 559.1396 - mae: 512.3618 - val_loss: 582.2488 - val_mae: 535.5272\n",
            "Epoch 78/2000\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 558.5491 - mae: 511.8485 - val_loss: 576.2811 - val_mae: 529.4713\n",
            "NN score: 541.94334438\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IkXtmCnwvNxc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mukw1BHzvN04"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFYjO66Vs3np"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}