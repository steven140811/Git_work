Thinking1	什么是反向传播中的链式法则
Answer1    误差是层层传递过来的，通过把权重矩阵做转置再乘以输出层的误差的方式，把误差一层一层的反向向前传播

Thinking1	请列举几种常见的激活函数，激活函数有什么作用
Answer2    常用得激活函数有sigmoid、tanh、relu，激活函数都是非线性的，通过激活函数可以让神经网络的表达能力更强，不再是输入的线性组合，而是几乎可以逼近任意函数
           
Thinking2	利用梯度下降法训练神经网络，发现模型loss不变，可能有哪些问题？怎么解决？
Answer3    可能是在深层模型中采用了sigmoid作为激活函数，当梯度小于1时，会发生梯度消失，
           解决的方法是可以把激活函数改为relu，因为relu函数由于非负区间的梯度为常数，因此不存在梯度消失问题，使得模型的收敛速度维持在一个稳定状态

